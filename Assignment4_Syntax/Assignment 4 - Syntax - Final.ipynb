{"cells":[{"cell_type":"markdown","id":"c5842d2f","metadata":{"id":"c5842d2f"},"source":["# Assignment 4 - Syntax\n","\n","This assigment has two parts. The first part is a continuation of assigment 2 about text classification. You will develop a text classifier based on syntactic features, namely POS and dependency, and test it on the 20 newsgroup dataset. The second part will focus on context free grammar and constituency parsing.  \n"]},{"cell_type":"markdown","id":"EhbOVbHgEZcN","metadata":{"id":"EhbOVbHgEZcN"},"source":["## Part 1 - Text Classification with Syntactic Features"]},{"cell_type":"markdown","id":"db16ab35","metadata":{"id":"db16ab35"},"source":["First, let's install Spacy and download the English pipeline."]},{"cell_type":"code","execution_count":1,"id":"Bc1p1lcRXPAv","metadata":{"id":"Bc1p1lcRXPAv"},"outputs":[],"source":["import spacy\n","\n","# !python -m spacy download en_core_web_lg"]},{"cell_type":"markdown","id":"7b548499","metadata":{"id":"7b548499"},"source":["We are now loading the 20 newsgroup dataset, as we did in assignment 2. Again, we will only keep two topics, 'talk.politics.misc' and 'talk.religion.misc'."]},{"cell_type":"code","execution_count":2,"id":"4dc4d1f2","metadata":{"id":"4dc4d1f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed 21 empty documents from the train set. Before: 842. After: 821.\n","Removed 8 empty documents from the test set. Before: 561. After: 553.\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_train = fetch_20newsgroups(\n","    subset='train', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","newsgroups_test = fetch_20newsgroups(\n","    subset='test', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","# Remove empty documents\n","for s, name in zip([newsgroups_train, newsgroups_test], [\"train\", \"test\"]):\n","  empty_indices = {i for i, doc in enumerate(s.data) if len(doc) == 0}\n","  orig_len = len(s.data)\n","  for k in ['data', 'filenames', 'target', 'DESCR']:\n","    s[k] = [s[k][i] for i in range(orig_len) if i not in empty_indices]\n","  print(f\"Removed {len(empty_indices)} empty documents from the {name} set. Before: {orig_len}. After: {len(s.data)}.\")"]},{"cell_type":"markdown","id":"W1HmNHSSNoWJ","metadata":{"id":"W1HmNHSSNoWJ"},"source":["Here we explore how to vectorize documents with two syntatic features: part-of-speech tags and syntactic dependencies, in addition to the bag-of-words features (same as assignment 2: lower-cased lemmas, no punctuation or stop words).\n","\n","We will need to build the pipeline for the syntax-based classifier. Please complete the code below to implement the syntax-based document vectorizer __SyntaxVectorizer__. This vectorizer will extract POS and dependency features and represent each of them as a bag-of-words."]},{"cell_type":"code","execution_count":9,"id":"ad98ae8d","metadata":{"id":"ad98ae8d"},"outputs":[],"source":["import string\n","import numpy as np\n","\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import BaseEstimator\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from tqdm import tqdm\n","\n","\n","class SyntaxVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.pos_vectorizer = CountVectorizer()\n","        self.dep_vectorizer = CountVectorizer()\n","        self.word_vectorizer = CountVectorizer()\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        # Make a set of all documents\n","        lemma_features, dep_features, pos_features = [], [], []\n","        for i, sentence in tqdm(enumerate(X), total=len(X)):\n","            tokens = self.nlp(sentence.lower())  # lower case and tokenize\n","            # remove stopwords and punctuations,\n","            tokens = [token for token in tokens if token.text not in STOP_WORDS and not token.is_punct]\n","            lemma_features.append(\" \".join([token.lemma_ for token in tokens])) \n","            dep_features.append(\" \".join([token.dep_ for token in tokens])) \n","            pos_features.append(\" \".join([token.pos_ for token in tokens])) \n","        ####################################\n","\n","        # Gets a list of documents, each document represented as string\n","        # concatenation of all POS (dep) tags, and transforms them into\n","        # count vectors (bag-of-features).\n","        X_pos = self.pos_vectorizer.transform(pos_features).toarray()\n","        X_dep = self.dep_vectorizer.transform(dep_features).toarray()\n","        X_word = self.word_vectorizer.transform(lemma_features).toarray()\n","\n","        X_features = np.concatenate((X_pos, X_dep, X_word), axis=-1)\n","        return X_features\n","\n","    def fit(self, X, y=None):\n","        self.pos_vectorizer.fit(X)\n","        self.dep_vectorizer.fit(X)\n","        self.word_vectorizer.fit(X)\n","        return self\n","\n","\n","# Load the English pipeline en_core_web_lg\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","# Create pipeline for the syntax-based classifier\n","classifier = LogisticRegression(max_iter=1000)\n","pipe = Pipeline([\n","    (\"vectorizer\", SyntaxVectorizer(nlp)), (\"classifier\", classifier)])"]},{"cell_type":"markdown","id":"esirELD7hbBn","metadata":{"id":"esirELD7hbBn"},"source":["Now let's train this classifier."]},{"cell_type":"code","execution_count":10,"id":"6obMVRXsg_sF","metadata":{"id":"6obMVRXsg_sF"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 821/821 [00:35<00:00, 23.23it/s]\n"]},{"data":{"text/html":["<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 SyntaxVectorizer(nlp=&lt;spacy.lang.en.English object at 0x00000204272B92B0&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 SyntaxVectorizer(nlp=&lt;spacy.lang.en.English object at 0x00000204272B92B0&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SyntaxVectorizer</label><div class=\"sk-toggleable__content\"><pre>SyntaxVectorizer(nlp=&lt;spacy.lang.en.English object at 0x00000204272B92B0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"],"text/plain":["Pipeline(steps=[('vectorizer',\n","                 SyntaxVectorizer(nlp=<spacy.lang.en.English object at 0x00000204272B92B0>)),\n","                ('classifier', LogisticRegression(max_iter=1000))])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["pipe.fit(newsgroups_train.data, newsgroups_train.target)"]},{"cell_type":"markdown","id":"ezKtwv7Ihp-s","metadata":{"id":"ezKtwv7Ihp-s"},"source":["Finally, let's predict and evaluate the test set. How does the performance compare to our BOW and CBOW classifiers from assignment 2?"]},{"cell_type":"code","execution_count":11,"id":"FNQodX31m1FC","metadata":{"id":"FNQodX31m1FC"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 553/553 [00:22<00:00, 24.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 74.68%\n"]}],"source":["from sklearn import metrics\n","\n","preds = pipe.predict(newsgroups_test.data)\n","print(f\"Logistic Regression Accuracy: {metrics.accuracy_score(newsgroups_test.target, preds)*100:.2f}%\")"]},{"cell_type":"markdown","id":"bQjLUqYuEggm","metadata":{"id":"bQjLUqYuEggm"},"source":["## Part 2 - Context Free Grammar\n","\n","In this part, you will be working with constituency parsing with structures defined by context-free grammars.\n","\n","Specifically, you will first convert a set of production rules into Chomsky Normal Form, before applying the Cocke–Kasami-Younger (CKY) Algorithm to obatin all possible parses for a given input sentence."]},{"cell_type":"markdown","id":"PMC4f_gBEwwl","metadata":{"id":"PMC4f_gBEwwl"},"source":["### Chomsky Normal Form\n","\n","The following function takes a list of production rules and converts them into Chomsky Normal Forms (CNF). As a reminder, in CNF, a non-terminal can generate:\n","\n","1. A terminal (e.g.; X -> x); or\n","2. Two non-terminals (e.g.; X->YZ).\n","\n","Complete `convert_rule` to generate new rules for the following cases:\n","\n","1. A terminal generates more than two (non-)terminals, e.g. `A -> B C D`. In that case, split it to `A -> A0 D` and `A0 -> B C`, recursively checking `A -> A0 D`. Don't forget to add the new non-terminal to `non_terminals`.\n","2. A terminal generates more a mix of terminals and non-terminals, e.g. `A -> B b`. In that case, split it to `A -> B A1`, and `A1 -> b`, recursively checking `A -> B A1`. Don't forget to add the new non-terminal to `non_terminals`."]},{"cell_type":"code","execution_count":12,"id":"DMQDg7YFEwwm","metadata":{"id":"DMQDg7YFEwwm"},"outputs":[],"source":["from collections import defaultdict\n","\n","\n","def convert_rule(rule, non_terminals):\n","  \"\"\"\n","  Gets a rule (as a tuple) and returns a list of rules to replace it\n","  \"\"\"\n","  # A terminal generates a non-terminal (e.g. A -> a), return it\n","  if len(rule) == 2 and rule[1][0] == \"'\":\n","    return [tuple(rule)]\n","\n","  # A terminal generates two non-terminals (e.g. A -> B C), return it.\n","  # We also return unary rules (e.g. A -> B) which will be taken care of later.\n","  elif len(rule) <= 3 and all([x[0] != \"'\" for x in rule]):\n","    return [tuple(rule)]\n","\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  # either a mix of 1 non-terminal and 1 terminal,  or more than 2 (non)terminals \n","  #                    (e.g. A ->  a  C)                 (e.g. A ->  B C D)\n","  elif len(rule) >= 3:\n","    first = rule[0]\n","    last = rule[-1]\n","    middle = rule[1:-1]\n","\n","\n","    # create the new non-terminal based on the existing non-terminals\n","    new_non_terminal = first\n","    index = 0\n","    while new_non_terminal in non_terminals:\n","      new_non_terminal = f\"{first}{index}\"\n","      index += 1\n","    non_terminals.append(new_non_terminal)\n","\n","    # for when the terminal is the last (e.g. A -> C a)\n","    if len(rule) == 3 and (last[0]==\"'\"):\n","      # converted_rule = [tuple([first,rule[1], new_non_terminal])] + convert_rule([new_non_terminal, last], non_terminals)\n","      converted_rule = convert_rule([first,rule[1], new_non_terminal], non_terminals) + convert_rule([new_non_terminal, last], non_terminals)\n","    else:  \n","      # for when the terminal is in the middle like A ->  a  C\n","      # or when we have more than 3 non terminals overal,  e.g. A ->  B C D\n","      # or for when we have more ethan 3, and mix again, but the terminal is last, e.g. A -> B C d. This is why the first item bellow is convert(), and not just a tuple\n","      converted_rule = convert_rule([first,new_non_terminal, last], non_terminals) + convert_rule([new_non_terminal] + middle, non_terminals)\n","      # [tuple([first,new_non_terminal, last])] + convert_rule([new_non_terminal] + middle, non_terminals)\n","    return converted_rule\n","  ####################################\n","\n","\n","def convert_rules(grammar_rules):\n","    \"\"\"\n","    Converts a list context-free grammar rules into the\n","    formatted Chomsky Normal Form.\n","    \"\"\"\n","    grammar_rules = [rule.replace(\"->\", \"\").split() for rule in grammar_rules]\n","    non_terminals = list({rule[0] for rule in grammar_rules})\n","    new_rules = [convert_rule(rule, non_terminals) for rule in grammar_rules]\n","    new_rules = [rule for rule_lst in new_rules for rule in rule_lst]\n","\n","    # Recursively combine the unary rules (e.g. A -> B)\n","    # with an existing rule (if possible). E.g. if B -> b and B -> C D,\n","    # we will add A -> b and A -> C D to the rules.\n","    unary_rules = {rule for rule in new_rules\n","                   if len(rule) == 2 and rule[1][0] != \"'\"}\n","\n","    # Convert to a dictionary from the LHS to the rest of the rule,\n","    # for better processing of unary rules\n","    rules_dict = defaultdict(list)\n","    [rules_dict[rule[0]].append(list(rule[1:])) for rule in new_rules if rule not in unary_rules]\n","\n","    for unary_rule in unary_rules:\n","      lhs, rhs = unary_rule\n","      rules_dict[lhs].extend(rules_dict[rhs])\n","\n","    formatted_rules = [tuple([lhs] + rhs)\n","                       for lhs, rhs_lst in rules_dict.items()\n","                       for rhs in rhs_lst]\n","\n","    return formatted_rules"]},{"cell_type":"markdown","id":"45W7UQzfEwwn","metadata":{"id":"45W7UQzfEwwn"},"source":["You can test your implementation with the following code:"]},{"cell_type":"code","execution_count":13,"id":"8kessP7CEwwo","metadata":{"id":"8kessP7CEwwo"},"outputs":[],"source":["grammar_rules = [\n","    \"A -> B\",\n","    \"A -> B C\",\n","    \"A -> 'a'\",\n","    \"A -> B C D\",\n","    \"A -> C D E\",\n","    \"B -> C D E\",\n","    \"C -> D\",\n","    \"D -> 'b'\",\n","]\n","\n","assert set(convert_rules(grammar_rules)) == set([\n","    ('A', 'B', 'C'),\n","    ('A', \"'a'\"),\n","    ('A', 'A0', 'D'), # from A -> B C D\n","    ('A0', 'B', 'C'), # from A -> B C D\n","    ('A', 'A1', 'E'), # from A -> C D E\n","    ('A1', 'C', 'D'), # from A -> C D E\n","    ('B', 'B0', 'E'), # from B -> C D E\n","    ('B0', 'C', 'D'), # from B -> C D E\n","    ('D', \"'b'\"),\n","    ('C', \"'b'\"), # from C -> D and D -> b\n","    ('A', 'B0', 'E'), # from A -> B and B -> C D E\n","])"]},{"cell_type":"markdown","id":"oEp4Ktw4Ewwp","metadata":{"id":"oEp4Ktw4Ewwp"},"source":["### CKY Parsing\n","\n","The following function outputs all possible parses based on the input text and a list of CNF production rules.\n","\n","Each entry in the cell is represented by the `Node` object, which stores the left and right child of the non-terminal (only left child when generated from terminals). This is to keep track of the entry from which it was derived, and is used to recursively retrieve its component constituents.\n","\n","Using CKY, each cell in the parse triangle can be filled in $\\mathcal{O}(n)$ time, where the number of spans to consider is defined by the variable `n_spans`, where higher levels considers more spans. Please complete in the missing code where indicated by assigning the left and right cell according to the appropriate indices in the parse triangle, so the existing cell considers all places where the input might be split in two."]},{"cell_type":"code","execution_count":14,"id":"AKsIpZLEEwwp","metadata":{"id":"AKsIpZLEEwwp"},"outputs":[],"source":["class Node(object):\n","    \"\"\"\n","    Data structure used for storing information about a non-terminal symbol.\n","    \"\"\"\n","    def __init__(self, symbol, child1, child2=None):\n","        self.symbol = symbol\n","        self.child1 = child1\n","        self.child2 = child2\n","\n","    def __repr__(self):\n","        return self.symbol\n","\n","\n","def CKY_parse(text, rules):\n","    \"\"\"\n","    Performs Constituency Parsing with the CKY algorithm.\n","\n","    Args:\n","        text (str): Input sentence, where terminals are separated by white spaces\n","        rules (List[List[str]]): List of production rules in CNF (see `simple_CNF` for format)\n","\n","    Returns:\n","        parse_triangle (List[List[List[Node]]]): Data structure for CKY parsing,\n","        the first row `parse_triangle[0]` represents the non-terminals for\n","        generating the terminals in the input text, where each cell\n","        `parse_triangle[i][j]` is represented by a list containing all possible\n","        symbols based on the previous cells.\n","    \"\"\"\n","    tokens = text.split()\n","    length = len(tokens)\n","\n","    # Data structure for storing the subtrees\n","    parse_triangle = [[[] for x in range(length - i)] for i in range(length)]\n","\n","    for i, tok in enumerate(tokens):\n","\n","        # Find out which non-terminals can generate the terminals\n","        # in the input text and put them into the parse table.\n","        # One terminal could be generated by multiple non-terminals,\n","        # therefore the parse table will contain a list of non terminals.\n","        for rule in rules:\n","            if f\"'{tok}'\" == rule[1]:\n","                parse_triangle[0][i].append(Node(rule[0], tok))\n","\n","    # For each span length from 2 to the entire string\n","    for row_idx in range(1, length):\n","\n","        # For each start index\n","        for cell_idx in range(length - row_idx):\n","\n","            # For each partition index\n","            for span_idx in range(row_idx):\n","\n","                # Index the parse_triangle for the left and right cell to consider\n","                # (hint: the indices is based on `row_idx`, `cell_idx`, and `span_idx`)\n","\n","                ####################################\n","                #   Your code here\n","                ####################################\n","                left_cell = parse_triangle[span_idx][cell_idx]\n","                right_cell = parse_triangle[row_idx - span_idx - 1][cell_idx + span_idx + 1]\n","                ####################################\n","                \n","                # For any rule where a non-terminal can be produced from the symbols\n","                # in the left and right cell\n","                for rule in rules:\n","                    if len(rule) == 3:\n","\n","                        # Update the current cell based if a non-terminal can be produced.\n","\n","                        ####################################\n","                        #   Your code here\n","                        ####################################\n","                        left_nodes = [node for node in left_cell if node.symbol == rule[1]]\n","                        right_nodes = [node for node in right_cell if node.symbol == rule[2]]\n","                        ####################################\n","                        parse_triangle[row_idx][cell_idx].extend(\n","                            [Node(rule[0], left, right) for left in left_nodes for right in right_nodes]\n","                        )\n","\n","    return parse_triangle"]},{"cell_type":"markdown","id":"aJC8fuPlWqOf","metadata":{"id":"aJC8fuPlWqOf"},"source":["Now we can test our CKY implementation. The function `print_tree` prints out all possible parse trees using recursion given the `parse_triangle` data structure returned by the `CKY_parse` function."]},{"cell_type":"code","execution_count":15,"id":"8KaZsGqCLy2E","metadata":{"id":"8KaZsGqCLy2E"},"outputs":[],"source":["def generate_tree(node):\n","    \"\"\"\n","    Generates the string representation of the parse tree.\n","    :param node: the root node.\n","    :return: the parse tree in string form.\n","    \"\"\"\n","    if node.child2 is None:\n","        return f\"[{node.symbol} '{node.child1}']\"\n","    return f\"[{node.symbol} {generate_tree(node.child1)} {generate_tree(node.child2)}]\"\n","\n","def print_tree(parsed_triangle, rules, output=True):\n","    \"\"\"\n","    Print the parse tree starting with the start symbol. Alternatively it returns the string\n","    representation of the tree(s) instead of printing it.\n","    \"\"\"\n","    start_symbol = rules[0][0]\n","    final_nodes = [n for n in parsed_triangle[-1][0] if n.symbol == start_symbol]\n","    print(\"Possible parse(s):\")\n","    trees = [generate_tree(node) for node in final_nodes]\n","    if output:\n","        for tree in trees:\n","            print(tree)"]},{"cell_type":"markdown","id":"uKIoNRhEXFsE","metadata":{"id":"uKIoNRhEXFsE"},"source":["Let's combine everything we implemented in this part to test our constituency parsing algorithm. Given the list of CFG production rules `grammar_rules` and a text input `text_input`, print out all possible parse trees using the functions defined in the previous questions."]},{"cell_type":"code","execution_count":17,"id":"AXSF9CmXXBS7","metadata":{"id":"AXSF9CmXXBS7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Possible parse(s):\n","[S [NP 'I'] [VP [V 'shot'] [NP [NP0 [Det 'an'] [N 'elephant']] [PP [P 'in'] [NP [Det 'my'] [N 'pajamas']]]]]]\n","[S [NP 'I'] [VP [VP [V 'shot'] [NP [Det 'an'] [N 'elephant']]] [PP [P 'in'] [NP [Det 'my'] [N 'pajamas']]]]]\n"]}],"source":["grammar_rules = [\n","    \"S -> NP VP\",\n","    \"PP -> P NP\",\n","    \"NP -> Det N\",\n","    \"NP -> Det N PP\",\n","    \"VP -> V NP\",\n","    \"VP -> VP PP\",\n","    \"NP -> 'I'\",\n","    \"Det -> 'an'\",\n","    \"Det -> 'my'\",\n","    \"N -> 'elephant'\",\n","    \"N -> 'pajamas'\",\n","    \"V -> 'shot'\",\n","    \"P -> 'in'\",\n","]\n","\n","text_input = \"I shot an elephant in my pajamas\"\n","\n","####################################\n","#   Your code here\n","####################################\n","formatted_rules = convert_rules(grammar_rules)\n","parse_triangle = CKY_parse(text=text_input, rules=formatted_rules)\n","print_tree(parsed_triangle=parse_triangle, rules=formatted_rules)\n","####################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"notebook","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
