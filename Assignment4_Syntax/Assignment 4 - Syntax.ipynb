{"cells":[{"cell_type":"markdown","id":"c5842d2f","metadata":{"id":"c5842d2f"},"source":["# Assignment 4 - Syntax\n","\n","This assigment has two parts. The first part is a continuation of assigment 2 about text classification. You will develop a text classifier based on syntactic features, namely POS and dependency, and test it on the 20 newsgroup dataset. The second part will focus on context free grammar and constituency parsing.  \n"]},{"cell_type":"markdown","id":"EhbOVbHgEZcN","metadata":{"id":"EhbOVbHgEZcN"},"source":["## Part 1 - Text Classification with Syntactic Features"]},{"cell_type":"markdown","id":"db16ab35","metadata":{"id":"db16ab35"},"source":["First, let's install Spacy and download the English pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"Bc1p1lcRXPAv","metadata":{"id":"Bc1p1lcRXPAv"},"outputs":[],"source":["import spacy\n","\n","# !python -m spacy download en_core_web_lg"]},{"cell_type":"markdown","id":"7b548499","metadata":{"id":"7b548499"},"source":["We are now loading the 20 newsgroup dataset, as we did in assignment 2. Again, we will only keep two topics, 'talk.politics.misc' and 'talk.religion.misc'."]},{"cell_type":"code","execution_count":null,"id":"4dc4d1f2","metadata":{"id":"4dc4d1f2"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_train = fetch_20newsgroups(\n","    subset='train', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","newsgroups_test = fetch_20newsgroups(\n","    subset='test', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","# Remove empty documents\n","for s, name in zip([newsgroups_train, newsgroups_test], [\"train\", \"test\"]):\n","  empty_indices = {i for i, doc in enumerate(s.data) if len(doc) == 0}\n","  orig_len = len(s.data)\n","  for k in ['data', 'filenames', 'target', 'DESCR']:\n","    s[k] = [s[k][i] for i in range(orig_len) if i not in empty_indices]\n","  print(f\"Removed {len(empty_indices)} empty documents from the {name} set. Before: {orig_len}. After: {len(s.data)}.\")"]},{"cell_type":"markdown","id":"W1HmNHSSNoWJ","metadata":{"id":"W1HmNHSSNoWJ"},"source":["Here we explore how to vectorize documents with two syntatic features: part-of-speech tags and syntactic dependencies, in addition to the bag-of-words features (same as assignment 2: lower-cased lemmas, no punctuation or stop words).\n","\n","We will need to build the pipeline for the syntax-based classifier. Please complete the code below to implement the syntax-based document vectorizer __SyntaxVectorizer__. This vectorizer will extract POS and dependency features and represent each of them as a bag-of-words."]},{"cell_type":"code","execution_count":null,"id":"ad98ae8d","metadata":{"id":"ad98ae8d"},"outputs":[],"source":["import string\n","import numpy as np\n","\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import BaseEstimator\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","class SyntaxVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.pos_vectorizer = CountVectorizer()\n","        self.dep_vectorizer = CountVectorizer()\n","        self.word_vectorizer = CountVectorizer()\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        # docs = [self.nlp(sent.lower()) for sent in X] # lower case and tokenize\n","        # lemma_features = [token.lemma_ for tokens in docs for token in tokens if token.text not in STOP_WORDS and not token.is_punct]\n","        # dep_features = [token.dep_ for tokens in docs for token in tokens if token.text not in STOP_WORDS and not token.is_punct]\n","        # pos_features = [token.pos_ for tokens in docs for token in tokens if token.text not in STOP_WORDS and not token.is_punct]\n","\n","        # # more efficiently\n","        lemma_features, dep_features, pos_features = [], [], []\n","        # docs = [self.nlp(sent.lower()) for sent in X] # lower case and tokenize\n","        for doc in X:\n","            tokens = self.nlp(doc.lower() if isinstance(doc, str) else doc)  # lower case and tokenize\n","            tokens = [token for token in tokens if token.text not in STOP_WORDS and not token.is_punct]  # remove stopwords and punctuations, and then lemmatize\n","            lemma_features.append(\" \".join(token.lemma_ for token  in tokens))\n","            dep_features.append(\" \".join(token.dep_ for token in tokens))\n","            pos_features.append(\" \".join(token.pos_ for token in tokens))\n","            # for token in tokens:\n","            #     lemma_features.append(token.lemma_)\n","            #     dep_features.append(token.dep_)\n","            #     pos_features.append(token.pos_)\n","                # features.append([token.lemma_, token.dep_, token.pos_])\n","        ####################################\n","        # Gets a list of documents, each document represented as string\n","        # concatenation of all POS (dep) tags, and transforms them into\n","        # count vectors (bag-of-features).\n","        X_pos = self.pos_vectorizer.transform(pos_features).toarray()\n","        X_dep = self.dep_vectorizer.transform(dep_features).toarray()\n","        X_word = self.word_vectorizer.transform(lemma_features).toarray()\n","\n","        X_features = np.concatenate((X_pos, X_dep, X_word), axis=-1)\n","        return X_features\n","\n","    def fit(self, X, y=None):\n","        self.pos_vectorizer.fit(X)\n","        self.dep_vectorizer.fit(X)\n","        self.word_vectorizer.fit(X)\n","        return self\n","\n","\n","# Load the English pipeline en_core_web_lg\n","nlp = spacy.load(\"en_core_web_lg\")\n","\n","# Create pipeline for the syntax-based classifier\n","classifier = LogisticRegression(max_iter=1000)\n","pipe = Pipeline([\n","    (\"vectorizer\", SyntaxVectorizer(nlp)), (\"classifier\", classifier)])"]},{"cell_type":"markdown","id":"esirELD7hbBn","metadata":{"id":"esirELD7hbBn"},"source":["Now let's train this classifier."]},{"cell_type":"code","execution_count":null,"id":"6obMVRXsg_sF","metadata":{"id":"6obMVRXsg_sF"},"outputs":[],"source":["pipe.fit(newsgroups_train.data, newsgroups_train.target)"]},{"cell_type":"markdown","id":"ezKtwv7Ihp-s","metadata":{"id":"ezKtwv7Ihp-s"},"source":["Finally, let's predict and evaluate the test set. How does the performance compare to our BOW and CBOW classifiers from assignment 2?"]},{"cell_type":"code","execution_count":null,"id":"FNQodX31m1FC","metadata":{"id":"FNQodX31m1FC"},"outputs":[],"source":["from sklearn import metrics\n","\n","preds = pipe.predict(newsgroups_test.data)\n","print(f\"Logistic Regression Accuracy: {metrics.accuracy_score(newsgroups_test.target, preds)*100:.2f}%\")"]},{"cell_type":"markdown","id":"bQjLUqYuEggm","metadata":{"id":"bQjLUqYuEggm"},"source":["## Part 2 - Context Free Grammar\n","\n","In this part, you will be working with constituency parsing with structures defined by context-free grammars.\n","\n","Specifically, you will first convert a set of production rules into Chomsky Normal Form, before applying the Cockeâ€“Kasami-Younger (CKY) Algorithm to obatin all possible parses for a given input sentence."]},{"cell_type":"markdown","id":"PMC4f_gBEwwl","metadata":{"id":"PMC4f_gBEwwl"},"source":["### Chomsky Normal Form\n","\n","The following function takes a list of production rules and converts them into Chomsky Normal Forms (CNF). As a reminder, in CNF, a non-terminal can generate:\n","\n","1. A terminal (e.g.; X -> x); or\n","2. Two non-terminals (e.g.; X->YZ).\n","\n","Complete `convert_rule` to generate new rules for the following cases:\n","\n","1. A terminal generates more than two (non-)terminals, e.g. `A -> B C D`. In that case, split it to `A -> A0 D` and `A0 -> B C`, recursively checking `A -> A0 D`. Don't forget to add the new non-terminal to `non_terminals`.\n","2. A terminal generates more a mix of terminals and non-terminals, e.g. `A -> B b`. In that case, split it to `A -> B A1`, and `A1 -> b`, recursively checking `A -> B A1`. Don't forget to add the new non-terminal to `non_terminals`."]},{"cell_type":"code","execution_count":1,"id":"DMQDg7YFEwwm","metadata":{"id":"DMQDg7YFEwwm"},"outputs":[],"source":["from collections import defaultdict\n","\n","\n","def convert_rule(rule, non_terminals):\n","  \"\"\"\n","  Gets a rule (as a tuple) and returns a list of rules to replace it\n","  \"\"\"\n","  # A terminal generates a non-terminal (e.g. A -> a), return it\n","  if len(rule) == 2 and rule[1][0] == \"'\":\n","    return [tuple(rule)]\n","\n","  # A terminal generates two non-terminals (e.g. A -> B C), return it.\n","  # We also return unary rules (e.g. A -> B) which will be taken care of later.\n","  elif len(rule) <= 3 and all([x[0] != \"'\" for x in rule]):\n","    return [tuple(rule)]\n","\n","  ####################################\n","  #   Your code here\n","  ####################################\n","    # A non-terminal generates more than two non-terminals (e.g. A -> B C D)\n","  elif len(rule) > 3 and all([x[0] != \"'\" for x in rule]):\n","    new_rule = rule[1:3]\n","    non_terminal = get_new_rule(non_terminals, rule[0])\n","    non_terminals.append(non_terminal)\n","    # replace the old rules with new non_terminal\n","    rule = [tuple([rule[0], non_terminal, *rule[3:]])]\n","    rules = convert_rule([non_terminal] + new_rule, non_terminals)\n","    rule += rules\n","    return tuple(rule)\n","  # A non-terminal generates a mix of terminal and non-terminals (e.g. A -> B 'b')  \n","  else:\n","    for i in range(len(rule)):\n","      if rule[i][0] == \"'\":\n","        #TODO: Check whether the new_rule has to be passed or the non_terminal\n","        new_rule = rule[i][1].upper().replace(\"'\", \"\")\n","        non_terminal = get_new_rule(non_terminals, new_rule)    \n","        non_terminals.append(non_terminal)\n","        rule = [(rule[0], non_terminal, rule[-1])]\n","        rules = convert_rule([rule[:i] + new_rule + rule[i+1:]], non_terminals)\n","        rule += rules\n","    return tuple(rule)\n","            \n","def get_new_rule(non_terminals, new_rule):\n","    assert new_rule[0] != \"'\", f\"{new_rule} is not a valid rule\"\n","    base = new_rule.rstrip(\"0123465789\") \n","    filtered = [non_terminal for non_terminal in non_terminals if non_terminal.startswith(base)]\n","    if len(filtered) == 0:\n","      assert len(new_rule) == len(base), f\"{new_rule} does not match {base}\"\n","      return new_rule\n","    elif len(filtered) == 1:\n","      assert len(new_rule) == len(base), f\"{new_rule} does not match {base}\"\n","      return f\"{base}0\"\n","    largest = int(filtered[-1][len(base):])\n","    return f\"{base}{largest+1}\"\n","\n","  ####################################\n","\n","\n","def convert_rules(grammar_rules):\n","    \"\"\"\n","    Converts a list context-free grammar rules into the\n","    formatted Chomsky Normal Form.\n","    \"\"\"\n","    grammar_rules = [rule.replace(\"->\", \"\").split() for rule in grammar_rules]\n","    non_terminals = list({rule[0] for rule in grammar_rules})\n","    new_rules = [convert_rule(rule, non_terminals) for rule in grammar_rules]\n","    new_rules = [rule for rule_lst in new_rules for rule in rule_lst]\n","\n","    # Recursively combine the unary rules (e.g. A -> B)\n","    # with an existing rule (if possible). E.g. if B -> b and B -> C D,\n","    # we will add A -> b and A -> C D to the rules.\n","    unary_rules = {rule for rule in new_rules\n","                   if len(rule) == 2 and rule[1][0] != \"'\"}\n","\n","    # Convert to a dictionary from the LHS to the rest of the rule,\n","    # for better processing of unary rules\n","    rules_dict = defaultdict(list)\n","    [rules_dict[rule[0]].append(list(rule[1:])) for rule in new_rules if rule not in unary_rules]\n","\n","    for unary_rule in unary_rules:\n","      lhs, rhs = unary_rule\n","      rules_dict[lhs].extend(rules_dict[rhs])\n","\n","    formatted_rules = [tuple([lhs] + rhs)\n","                       for lhs, rhs_lst in rules_dict.items()\n","                       for rhs in rhs_lst]\n","\n","    return formatted_rules"]},{"cell_type":"markdown","id":"45W7UQzfEwwn","metadata":{"id":"45W7UQzfEwwn"},"source":["You can test your implementation with the following code:"]},{"cell_type":"code","execution_count":2,"id":"8kessP7CEwwo","metadata":{"id":"8kessP7CEwwo"},"outputs":[],"source":["grammar_rules = [\n","    \"A -> B\",\n","    \"A -> B C\",\n","    \"A -> 'a'\",\n","    \"A -> B C D\",\n","    \"A -> C D E\",\n","    \"B -> C D E\",\n","    \"C -> D\",\n","    \"D -> 'b'\",\n","]\n","\n","assert set(convert_rules(grammar_rules)) == set([\n","    ('A', 'B', 'C'),\n","    ('A', \"'a'\"),\n","    ('A', 'A0', 'D'), # from A -> B C D\n","    ('A0', 'B', 'C'), # from A -> B C D\n","    ('A', 'A1', 'E'), # from A -> C D E\n","    ('A1', 'C', 'D'), # from A -> C D E\n","    ('B', 'B0', 'E'), # from B -> C D E\n","    ('B0', 'C', 'D'), # from B -> C D E\n","    ('D', \"'b'\"),\n","    ('C', \"'b'\"), # from C -> D and D -> b\n","    ('A', 'B0', 'E'), # from A -> B and B -> C D E\n","])"]},{"cell_type":"markdown","id":"oEp4Ktw4Ewwp","metadata":{"id":"oEp4Ktw4Ewwp"},"source":["### CKY Parsing\n","\n","The following function outputs all possible parses based on the input text and a list of CNF production rules.\n","\n","Each entry in the cell is represented by the `Node` object, which stores the left and right child of the non-terminal (only left child when generated from terminals). This is to keep track of the entry from which it was derived, and is used to recursively retrieve its component constituents.\n","\n","Using CKY, each cell in the parse triangle can be filled in $\\mathcal{O}(n)$ time, where the number of spans to consider is defined by the variable `n_spans`, where higher levels considers more spans. Please complete in the missing code where indicated by assigning the left and right cell according to the appropriate indices in the parse triangle, so the existing cell considers all places where the input might be split in two."]},{"cell_type":"code","execution_count":9,"id":"AKsIpZLEEwwp","metadata":{"id":"AKsIpZLEEwwp"},"outputs":[],"source":["class Node(object):\n","    \"\"\"\n","    Data structure used for storing information about a non-terminal symbol.\n","    \"\"\"\n","    def __init__(self, symbol, child1, child2=None):\n","        self.symbol = symbol\n","        self.child1 = child1\n","        self.child2 = child2\n","\n","    def __repr__(self):\n","        return self.symbol\n","\n","\n","def CKY_parse(text, rules):\n","    \"\"\"\n","    Performs Constituency Parsing with the CKY algorithm.\n","\n","    Args:\n","        text (str): Input sentence, where terminals are separated by white spaces\n","        rules (List[List[str]]): List of production rules in CNF (see `simple_CNF` for format)\n","\n","    Returns:\n","        parse_triangle (List[List[List[Node]]]): Data structure for CKY parsing,\n","        the first row `parse_triangle[0]` represents the non-terminals for\n","        generating the terminals in the input text, where each cell\n","        `parse_triangle[i][j]` is represented by a list containing all possible\n","        symbols based on the previous cells.\n","    \"\"\"\n","    tokens = text.split()\n","    length = len(tokens)\n","\n","    # Data structure for storing the subtrees\n","    parse_triangle = [[[] for x in range(length - i)] for i in range(length)]\n","\n","    for i, tok in enumerate(tokens):\n","\n","        # Find out which non-terminals can generate the terminals\n","        # in the input text and put them into the parse table.\n","        # One terminal could be generated by multiple non-terminals,\n","        # therefore the parse table will contain a list of non terminals.\n","        for rule in rules:\n","            if f\"'{tok}'\" == rule[1]:\n","                parse_triangle[0][i].append(Node(rule[0], tok))\n","\n","    # For each span length from 2 to the entire string\n","    for row_idx in range(1, length):\n","\n","        # For each start index\n","        for cell_idx in range(length - row_idx):\n","\n","            # For each partition index\n","            for span_idx in range(row_idx):\n","\n","                # Index the parse_triangle for the left and right cell to consider\n","                # (hint: the indices is based on `row_idx`, `cell_idx`, and `span_idx`)\n","\n","                ####################################\n","                #   Your code here\n","                ####################################\n","                left_cell = parse_triangle[span_idx][cell_idx]\n","                right_cell = parse_triangle[row_idx - span_idx - 1][cell_idx + span_idx + 1]\n","                ####################################\n","\n","                # For any rule where a non-terminal can be produced from the symbols\n","                # in the left and right cell\n","                for rule in rules:\n","                    if len(rule) == 3:\n","\n","                        # Update the current cell based if a non-terminal can be produced.\n","\n","                        ####################################\n","                        #   Your code here\n","                        ####################################\n","                        left_nodes = [node for node in left_cell if node.symbol == rule[1]]\n","                        right_nodes = [node for node in right_cell if node.symbol == rule[2]]\n","                        ####################################\n","                        parse_triangle[row_idx][cell_idx].extend(\n","                            [Node(rule[0], left, right) for left in left_nodes for right in right_nodes]\n","                        )\n","\n","    return parse_triangle"]},{"cell_type":"markdown","id":"aJC8fuPlWqOf","metadata":{"id":"aJC8fuPlWqOf"},"source":["Now we can test our CKY implementation. The function `print_tree` prints out all possible parse trees using recursion given the `parse_triangle` data structure returned by the `CKY_parse` function."]},{"cell_type":"code","execution_count":10,"id":"8KaZsGqCLy2E","metadata":{"id":"8KaZsGqCLy2E"},"outputs":[],"source":["def generate_tree(node):\n","    \"\"\"\n","    Generates the string representation of the parse tree.\n","    :param node: the root node.\n","    :return: the parse tree in string form.\n","    \"\"\"\n","    if node.child2 is None:\n","        return f\"[{node.symbol} '{node.child1}']\"\n","    return f\"[{node.symbol} {generate_tree(node.child1)} {generate_tree(node.child2)}]\"\n","\n","def print_tree(parsed_triangle, rules, output=True):\n","    \"\"\"\n","    Print the parse tree starting with the start symbol. Alternatively it returns the string\n","    representation of the tree(s) instead of printing it.\n","    \"\"\"\n","    start_symbol = rules[0][0]\n","    final_nodes = [n for n in parsed_triangle[-1][0] if n.symbol == start_symbol]\n","    print(\"Possible parse(s):\")\n","    trees = [generate_tree(node) for node in final_nodes]\n","    if output:\n","        for tree in trees:\n","            print(tree)"]},{"cell_type":"markdown","id":"uKIoNRhEXFsE","metadata":{"id":"uKIoNRhEXFsE"},"source":["Let's combine everything we implemented in this part to test our constituency parsing algorithm. Given the list of CFG production rules `grammar_rules` and a text input `text_input`, print out all possible parse trees using the functions defined in the previous questions."]},{"cell_type":"code","execution_count":11,"id":"AXSF9CmXXBS7","metadata":{"id":"AXSF9CmXXBS7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Possible parse(s):\n","[S [NP 'I'] [VP [V 'shot'] [NP [NP0 [Det 'an'] [N 'elephant']] [PP [P 'in'] [NP [Det 'my'] [N 'pajamas']]]]]]\n","[S [NP 'I'] [VP [VP [V 'shot'] [NP [Det 'an'] [N 'elephant']]] [PP [P 'in'] [NP [Det 'my'] [N 'pajamas']]]]]\n"]}],"source":["grammar_rules = [\n","    \"S -> NP VP\",\n","    \"PP -> P NP\",\n","    \"NP -> Det N\",\n","    \"NP -> Det N PP\",\n","    \"VP -> V NP\",\n","    \"VP -> VP PP\",\n","    \"NP -> 'I'\",\n","    \"Det -> 'an'\",\n","    \"Det -> 'my'\",\n","    \"N -> 'elephant'\",\n","    \"N -> 'pajamas'\",\n","    \"V -> 'shot'\",\n","    \"P -> 'in'\",\n","]\n","\n","text_input = \"I shot an elephant in my pajamas\"\n","\n","####################################\n","#   Your code here\n","####################################\n","formatted_rules = convert_rules(grammar_rules)\n","parse_triangle = CKY_parse(text=text_input, rules=formatted_rules)\n","print_tree(parsed_triangle=parse_triangle, rules=formatted_rules)\n","####################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"notebook","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
