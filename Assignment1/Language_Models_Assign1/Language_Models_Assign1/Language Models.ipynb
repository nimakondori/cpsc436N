{"cells":[{"cell_type":"markdown","metadata":{"id":"0Rv18UaRljJX"},"source":["## CPSC 436N Assignment 1 - Language Models\n","\n","In this assignment you will implement both a count-based N-gram language model and a simple neural language model.\n","\n","For the neural model, familiarize yourself with pytorch: https://pytorch.org/tutorials/beginner/basics/intro.html\n"]},{"cell_type":"markdown","metadata":{"id":"h4otuXYsljJb"},"source":["\n","\n","Let's load a number of dependencies."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7t9hPaLcljJd"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","from itertools import count\n","from scipy.special import softmax\n","from torch.nn.utils import clip_grad_norm_\n","from collections import defaultdict, Counter\n","\n","# Check if GPU is available to pytorch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"markdown","metadata":{"id":"vhWxwA6JljJf"},"source":["Next, we implement the corpus and vocabulary objects. The vocabulary object has an index for each word type, and the corpus object reads a text corpus and returns the word type index for each word instance in the corpus. It is a list of lines.  "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"--IW-bTprKHJ"},"outputs":[],"source":["class Vocabulary(object):\n","    def __init__(self):\n","        self.word2idx = defaultdict(count(0).__next__)\n","\n","    def add_word(self, word):\n","        _ = self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","    def idx2word(self):\n","        return {i: w for w, i in self.word2idx.items()}\n","\n","\n","class Corpus(object):\n","    def __init__(self, path, n=2, vocab=None):\n","        # Only initialize for the train corpus.\n","        # Then, for the dev and test corpus, use the vocabulary\n","        # from the training set\n","        if vocab is None:\n","          self.vocab = Vocabulary()\n","        else:\n","          self.vocab = vocab\n","\n","        # Read the entire corpus to memory\n","        lines = open(path).readlines()\n","\n","        # \"Tokenize\" (split to words) and add the <start> tokens based on the\n","        # number of n-grams, and the <end> tokens, for each line.\n","        lines = [['<start>'] * (n-1) + l.split() + ['<end>'] for l in lines]\n","\n","        # Convert word instance to word type ID\n","        lines = [[self.vocab.word2idx[w] for w in l] for l in lines]\n","\n","        self.lines = lines"]},{"cell_type":"markdown","metadata":{"id":"bMz414GKS2hO"},"source":["Load the Penn Tree Bank train, dev, and test sets."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["train number of sentences: 314, vocab size: 755\n","dev number of sentences: 62, vocab size: 755\n","test number of sentences: 42, vocab size: 755\n"]}],"source":["train_mini = Corpus(\"data_mini/train.txt\", n=2)\n","dev_mini = Corpus(\"data_mini/dev.txt\", vocab=train_mini.vocab, n=2)\n","test_mini = Corpus(\"data_mini/test.txt\", vocab=train_mini.vocab, n=2)\n","\n","for s, name in zip([train_mini, dev_mini, test_mini], [\"train\", \"dev\", \"test\"]):\n","  print(f\"{name} number of sentences: {len(s.lines)}, vocab size: {len(s.vocab)}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1689635643846,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"wmZBKmqjljJl","outputId":"6c7f675e-2c79-4a6e-fa84-9453424fe991"},"outputs":[{"name":"stdout","output_type":"stream","text":["train number of sentences: 25602, vocab size: 9473\n","dev number of sentences: 5120, vocab size: 9473\n","test number of sentences: 3413, vocab size: 9473\n"]}],"source":["train = Corpus(\"data/train.txt\")\n","dev = Corpus(\"data/dev.txt\", vocab=train.vocab)\n","test = Corpus(\"data/test.txt\", vocab=train.vocab)\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  print(f\"{name} number of sentences: {len(s.lines)}, vocab size: {len(s.vocab)}\")"]},{"cell_type":"markdown","metadata":{"id":"aSgcuuCAljJm"},"source":["### Bigram (Count-based) Language Model\n","\n","We will create a `CountBasedLM` object. Its training includes computing all the ngram counts in the corpus, for `n` and `n-1`. For inference you will need to complete the implementation of `compute_mle`."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"iuKg7_wqljJm"},"outputs":[],"source":["def get_ngrams(s, n):\n","  \"\"\"\n","  Gets a list of words and returns a list of n-grams\n","  \"\"\"\n","  return zip(*[s[i:] for i in range(n)])\n","\n","\n","class CountBasedLM(object):\n","    def __init__(self, n=2, laplace=1):\n","        self.vocab = []\n","        self.vocab_size = len(self.vocab)\n","        self.laplace = laplace\n","        self.n = n\n","\n","    def train(self, train_corpus):\n","        self.vocab = train_corpus.vocab\n","        self.vocab_size = len(self.vocab)\n","\n","        # Count the n-grams in the corpus\n","        words = [w for line in train_corpus.lines for w in line]\n","\n","        self.n_gram_count = dict(Counter(get_ngrams(words, self.n)))\n","        self.nm1_gram_count = dict(Counter(get_ngrams(words, self.n-1)))\n","\n","    def compute_mle(self, n_gram):\n","        \"\"\"\n","        Compute the MLE of P(w_i|w_{i-n+1}...w_{iâˆ’1}),\n","        with add-one Laplacian smoothing.\n","        Please see chapter 3.5.1 of J&M 3rd Ed. for more information.\n","        \"\"\"\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        # P(w_i|w_{i-1}) = Count(w_i, w{i-1}) + 1 / Count(w_{i-1}) + |V|\n","        num, denom = self.n_gram_count.get(n_gram, 0) + self.laplace, self.nm1_gram_count.get(n_gram[0:-1], 0) + self.laplace*self.vocab_size\n","\n","        prob = num / denom\n","        ####################################\n","\n","        return prob"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8YFIiHH-ljJy"},"outputs":[],"source":["bigram_lm = CountBasedLM()\n","bigram_lm.train(train)"]},{"cell_type":"markdown","metadata":{},"source":["### Q1: For the bigram model, what is P(agency | the) when the model is built with data_mini?  What is it when the model is built with data?\n","\n","data :      0.0019393651547534236\n","\n","data mini:  0.0011614401858304297"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.0019393651547534236\n","0.0011614401858304297\n"]}],"source":["print(bigram_lm.compute_mle((bigram_lm.vocab.word2idx[\"the\"], bigram_lm.vocab.word2idx[\"agency\"])))\n","\n","bigram_lm_mini = CountBasedLM()\n","bigram_lm_mini.train(train_mini)\n","print(bigram_lm_mini.compute_mle((bigram_lm_mini.vocab.word2idx[\"the\"], bigram_lm_mini.vocab.word2idx[\"agency\"])))"]},{"cell_type":"markdown","metadata":{"id":"7-NrVBqzpRyh"},"source":["Now, you will implement a function that uses the LM to compute the probability of a sentence based on the chain rule. There are several important implementation details:\n","\n","1. The function gets a (string) sentence so the first step should be to tokenize it and convert the tokens to token IDs.\n","\n","2. Work in log space to avoid underflow, but return the probability (not log probability).\n","\n","We will test this function by making sure that:\n","\n","* The probability of each sentence is between 0 and 1\n","\n","* The probability of a sentence is not higher when a word is added\n","\n","* The probability of a grammatical sentence is higher than that of an ungrammatical sentence."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2ISvoVuwpRi5"},"outputs":[],"source":["def compute_probability(lm, sentence):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  # Tokenize and add <start> and <end> tokens. \n","  # Note the usage of the lower() function. This is make sure the n_grams \n","  # match the keys in the vocab.\n","  if isinstance(sentence, str):\n","    tokens = ['<start>'] * (lm.n-1) + sentence.split() + ['<end>'] \n","    token_ids = [lm.vocab.word2idx[t] for t in tokens]\n","  else:\n","    token_ids = sentence\n","  # Get n_grams of the sentence\n","  n_grams = get_ngrams(token_ids, lm.n)\n","  probabilities = np.array([lm.compute_mle(n_gram) for n_gram in n_grams])\n","\n","  \n","  # take log, sum, and exp to get the probability+\n","  probability = np.exp(np.sum(np.log(probabilities)))\n","  # print(probability)\n","  ####################################\n","  return probability\n","\n","\n","s1 = \"This is a sentence\"\n","prob_s1 = compute_probability(bigram_lm, s1)\n","assert(0 <= prob_s1 <= 1)\n","\n","s2 = \"This is a longer sentence\"\n","prob_s2 = compute_probability(bigram_lm, s2)\n","assert(prob_s2 <= prob_s1)\n","\n","s3 = \"This longer is a sentence\"\n","prob_s3 = compute_probability(bigram_lm, s3)\n","assert(prob_s3 < prob_s2)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689635644895,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"hmsntk-6VYBV","outputId":"c67a4515-82f7-48b6-df04-488c690f7151"},"outputs":[{"name":"stdout","output_type":"stream","text":["The average train perplexity for the bigram LM: 73.67764504117923\n","The average dev perplexity for the bigram LM: 92.5652414291731\n","The average test perplexity for the bigram LM: 88.3328433862738\n"]}],"source":["def compute_perplexity(lm, corpus):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  # ngram_probs = np.array([compute_probability(lm, s) for s in corpus.lines])\n","  # Flatten the corpus to a list of words\n","  words = np.array([w for line in corpus.lines for w in line]) \n","  ngram_probs = np.array([lm.compute_mle(ngram) for ngram in get_ngrams(words, lm.n)])\n","  # Note: We only do this to match the expected output. The correct code should have the same base\n","  # and power terms. i.e.  np.power(2 ,np.sum(-np.log2(ngram_probs)) / len(ngram_probs)) \n","  perplexity = np.power(2 ,np.sum(-np.log(ngram_probs)) / len(ngram_probs))\n","  ####################################\n","  return perplexity      \n","\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  ppl = compute_perplexity(bigram_lm, s)\n","  print(f\"The average {name} perplexity for the bigram LM: {ppl}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1689635645298,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"kSUjRKSOwXCO","outputId":"bbb8b519-461f-457b-ba97-ddd1a799b154"},"outputs":[{"name":"stdout","output_type":"stream","text":["The average train perplexity for the bigram LM: 155.0793650719732\n","The average dev perplexity for the bigram LM: 221.4983370622599\n","The average test perplexity for the bigram LM: 215.23674252950858\n"]}],"source":["trigram_lm = CountBasedLM(n=3)\n","trigram_lm.train(train)\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  ppl = compute_perplexity(trigram_lm, s)\n","  print(f\"The average {name} perplexity for the bigram LM: {ppl}\")"]},{"cell_type":"markdown","metadata":{"id":"ZGuzE_TXVYPO"},"source":["We can now use the count-based LM to generate text. This is done by starting with an input `prompt`, computing the distribution for the next token, and sampling from it. We will implement `top k` decoding that prunes the distribution to the k most probable next tokens, re-normalizes it and samples from it proportionally to the probability for each token. Using `k=1` would enable greedy decoding, i.e. selecting the most probable next token at each time step. We will generate the text once with greedy decoding and 5 times with `k=1000`. Complete the following code."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84285,"status":"ok","timestamp":1689637850491,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"n95DYxr9GBDb","outputId":"9fcc9c05-7dd2-4a9c-bda3-662482046fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["k=1\n","<start> lawyers think the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n","k=2000\n","<start> lawyers think the computers memotec portfolio debt finance stake belongs reached in program\n","<start> lawyers think the problem trading six also underlying citizens body ltd. tells first\n","<start> lawyers think the federal adrs response yen slowly include acquisition after position food\n","<start> lawyers think the line-item handling drain floating-rate decades founder chip iii commodore led\n","<start> lawyers think the barrel belt voting block h. appealing senate traffic plus forces\n"]}],"source":["def generate_text_count_based(lm, prompt, k=1, max_tokens=10):\n","    # Tokenize and convert the sentence to a list of IDs\n","    tokens = ['<start>'] * (lm.n-1) + prompt.split()\n","    input_tokens = [lm.vocab.word2idx[t] for t in tokens]\n","\n","    # Iteratively generate the next word until generating <end>\n","    # or until reaching max_tokens.\n","    generated_tokens = []\n","\n","    for i in range(max_tokens):\n","      ####################################\n","      #   Your code here\n","      ####################################\n","      probs, all_tokens = {}, input_tokens + generated_tokens\n","      for word_idx, w in lm.vocab.idx2word().items():\n","        n_gram = tuple(all_tokens[-lm.n+1:] + [word_idx])\n","        probs[word_idx] = lm.compute_mle(n_gram)\n","      probs = np.array(sorted(probs.items(), key=lambda x: x[1], reverse=True)[:k])\n","      sum_probs = np.sum(probs[:,1])\n","      selected_index = np.random.choice(probs[:,0], p=probs[:,1]/sum_probs)\n","      ####################################\n","\n","      generated_tokens.append(selected_index)\n","\n","      if lm.vocab.idx2word()[selected_index] == \"<end>\":\n","        break\n","\n","    return \" \".join([lm.vocab.idx2word()[i] for i in input_tokens + generated_tokens])\n","\n","\n","print(\"k=1\")\n","print(generate_text_count_based(bigram_lm, \"lawyers think the\", k=1, max_tokens=10))\n","\n","print(\"k=2000\")\n","for _ in range(5):\n","  print(generate_text_count_based(bigram_lm, \"lawyers think the\", k=2000, max_tokens=10))"]},{"cell_type":"markdown","metadata":{"id":"p10QIUn9qasE"},"source":["### Neural Language Model\n","\n","The neural LM is an Ngram LM with one hidden layer as we learned in class. Let's start by defining some hyperparameters."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"99hAuwZaqasE"},"outputs":[],"source":["embed_size = 128\n","hidden_size = 1024\n","num_epochs = 2\n","learning_rate = 1e-2  # 2e-3 "]},{"cell_type":"markdown","metadata":{"id":"8cl7DhAIljJ0"},"source":["Now, you will implement the class for the neural ngram language model. You will complete the `forward` function that gets a tensor with the context (n-1) token IDs, and returns the unnormalized next token probability distribution."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"JZi51vWzljJ1"},"outputs":[],"source":["class NeuralNgramLM(nn.Module):\n","    def __init__(self, vocab, embed_size, intermediate_size, n=2):\n","        super(NeuralNgramLM, self).__init__()\n","        self.vocab = vocab\n","        self.vocab_size = len(self.vocab)\n","        self.n = n\n","        self.embed = nn.Embedding(self.vocab_size, embed_size)\n","        self.hidden = nn.Linear((self.n-1) * embed_size, hidden_size)\n","        self.output = nn.Linear(hidden_size, self.vocab_size)\n","\n","    def forward(self, x):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        y = self.output(self.hidden(self.embed(x)))\n","        ####################################\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"iFfNEeZrljJ1"},"source":["Next, let's create and train a neural bigram model."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"gK_WyjiOljJ1"},"outputs":[],"source":["neural_lm = NeuralNgramLM(train.vocab, embed_size, hidden_size).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(\n","    neural_lm.parameters(), lr=learning_rate, momentum=0.9)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71077,"status":"ok","timestamp":1689635724026,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"jb-yeBUKljJ2","outputId":"6e007d5b-54e9-495f-ade0-0f72f61bcdea"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 401/401 [00:33<00:00, 11.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch [1/2], Training Loss: 0.0194, Dev Perplexity: 22.77\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 401/401 [00:34<00:00, 11.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch [2/2], Training Loss: 0.0172, Dev Perplexity: 18.94\n","\n"]}],"source":["# Reduce batch size if you are running out of memory\n","batch_size = 64\n","\n","# Lets convert the corpora into tensors.\n","# For the training data, we will need to pad all the sentences\n","# to be in the same length because we are working in batches.\n","# But we don't need the same for the dev set.\n","pad_token = train.vocab.word2idx[\"<end>\"]\n","max_len = max([len(line) for line in train.lines])\n","padded = [line + (max_len - len(line) + 1) * [pad_token] for line in train.lines]\n","train_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in padded]\n","training_data = torch.cat(train_ids, dim=0).to(device)\n","\n","dev_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in dev.lines]\n","\n","neural_lm.train()\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for i in tqdm(range(0, len(training_data), batch_size)):\n","        batch = training_data[i:i + batch_size]\n","\n","        # Notice that we get a [batch_size, max_len-1] tensor\n","        # which would only work for a bigram LM.\n","        # If we want to train a n > 2 LM, we would need\n","        # add a dimension for the context.\n","        inputs = batch[:, :-1].to(device)\n","        targets = batch[:,1:].to(device).long()\n","\n","        # Forward pass\n","        # (the outputs shape should be [batch_size, max_len, hidden_size])\n","        outputs = neural_lm(inputs)\n","        loss = criterion(\n","            outputs.reshape(-1, neural_lm.vocab_size), targets.reshape(-1))\n","        total_loss += loss.item()\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Calculate the perplexity of the current trained model on\n","    # dev set. It should reduce between epochs. We will not\n","    # use our implementation of perplexity but instead\n","    # we will use cross-entropy loss. See J&M chapter 3.8\n","    # for the relationship between the two.\n","    total_ppl = 0\n","    for i in range(0, len(dev_ids)):\n","        dev_inputs = dev_ids[i][:, :-1].to(device)\n","        dev_targets = dev_ids[i][:,1:].to(device).long()\n","        dev_outputs = neural_lm(dev_inputs)\n","        ce = criterion(\n","            dev_outputs.reshape(-1, neural_lm.vocab_size),\n","            dev_targets.reshape(-1))\n","        total_ppl += ce.item();\n","\n","    print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], \" + \\\n","          f\"Training Loss: {total_loss/len(train_ids):.4f}, \" + \\\n","          f\"Dev Perplexity: {2**(total_ppl/len(dev_ids)):5.2f}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"sCbcvvDwljJ2"},"source":["Save the trained model."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"WEU68Mc0ljJ3"},"outputs":[],"source":["torch.save(neural_lm, f\"neural_{neural_lm.n}gram_lm.ckpt\")"]},{"cell_type":"markdown","metadata":{"id":"TRvZAdgSljJ3"},"source":["Finally, let's load the model and test it on the test set."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1647,"status":"ok","timestamp":1689635725658,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"UKu9d0lLljJ3","outputId":"7fecb390-6cca-485e-8fa7-f4007a4b0f9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Perplexity: 18.27\n","\n"]}],"source":["test_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in test.lines]\n","\n","neural_model = torch.load(f\"neural_{neural_lm.n}gram_lm.ckpt\")\n","neural_model.eval()\n","test_ppl = 0\n","with torch.no_grad():\n","    for i in range(0, len(test_ids)):\n","      inputs = test_ids[i][:, :-1].to(device)\n","      targets = test_ids[i][:,1:].to(device).long()\n","      outputs = neural_lm(inputs)\n","      ce = criterion(outputs.reshape(-1, neural_lm.vocab_size),\n","                     targets.reshape(-1))\n","      test_ppl += ce.item()\n","\n","print(f\"Test Perplexity: {2**(test_ppl/len(test_ids)):5.2f}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"WIQEXb6vTyul"},"source":["We can compute the probability of sentences using the neural LM. In fact, cross entropy loss between the first `|S|-1` tokens and the `|S|-1` last tokens is the negative log likelihood of a sequence, i.e. it is -1 * the average/sum of log probabilities for each token. Thanks to the monotonicity of log function, lower cross entropy loss corresponds to higer probability for a sentence. Since we are typically interested in comparing probabilities of sentences (rather than the absolute value), we can use CE loss."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"VhNuNs-IT8Pr"},"outputs":[{"name":"stdout","output_type":"stream","text":["-7.396886348724365\n","-8.764396667480469\n"]}],"source":["def compute_negative_log_likelihood(neural_lm, sentence):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  # Tokenize and add <start> and <end> tokens.\n","  # Note the usage of the lower() function. This is make sure the n_grams\n","  # match the keys in the vocab.\n","  neural_lm.eval()\n","  if isinstance(sentence, str):\n","    tokens = ['<start>'] * (neural_lm.n-1) + sentence.split() + ['<end>'] \n","    token_ids = [neural_lm.vocab.word2idx[t] for t in tokens]\n","  else:\n","    token_ids = sentence\n","  token_ids = torch.tensor(token_ids).to(device)\n","  output = neural_lm(token_ids[:-1])\n","  return criterion(output, token_ids[1:]).item()\n","  ####################################\n","\n","s1 = \"This is a sentence\"\n","prob_s1 = -compute_negative_log_likelihood(neural_lm, s1)\n","print(prob_s1)\n","\n","s2 = \"This is sentence a\"\n","prob_s2 = -compute_negative_log_likelihood(neural_lm, s2)\n","print(prob_s2)\n","assert(prob_s1 >= prob_s2)"]},{"cell_type":"markdown","metadata":{"id":"gnyiaeVc3SS9"},"source":["We can also use the neural LM to generate text. Complete the following code."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97659,"status":"ok","timestamp":1689638116004,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"ukTvd6oV5bAh","outputId":"911a4cdf-9897-467b-803a-889709e25c05"},"outputs":[{"name":"stdout","output_type":"stream","text":["k=1\n","<start> lawyers think the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n","k=2000\n","<start> lawyers think the bankamerica sharper illness offensive abandon abm functions certain incidents noranda\n","<start> lawyers think the seven-day ontario supercomputer seymour coup sansui criticism bit hope vehicle\n","<start> lawyers think the investment emphasized man queen assessed interest masters weakest heights ambitious\n","<start> lawyers think the acquisition connections jet construct catalyst occasions gerard mitsubishi operations subordinate\n","<start> lawyers think the sweeping catch ally persons kraft attacked centuries investigations coast actually\n"]}],"source":["def generate_text_neural(neural_lm, prompt, k=1, max_tokens=10):\n","    neural_lm.eval()\n","\n","    # Tokenize and convert the sentence to a list of IDs\n","    tokens = ['<start>'] * (neural_lm.n-1) + prompt.split()\n","    input_tokens = [neural_lm.vocab.word2idx[t] for t in tokens]\n","\n","    # Iteratively generate the next word until generating <end>\n","    # or until reaching max_tokens.\n","    generated_tokens = []\n","\n","    for i in range(max_tokens):\n","      token_ids = torch.tensor(input_tokens + generated_tokens, device=device)\n","\n","      ####################################\n","      #   Your code here\n","      ####################################\n","      probs = neural_lm(token_ids)    \n","      # Only want to get the last word's probability distribution\n","      top_probs, top_indices = torch.topk(probs[-1], k)\n","      # convert to numpy\n","      top_probs, top_indices = top_probs.detach().cpu().numpy(), top_indices.detach().cpu().numpy()\n","      # renormalize the probabilities of top-k items\n","      norm_probs = top_probs / np.sum(top_probs)\n","      selected_index = np.random.choice(top_indices, p=norm_probs)\n","      \n","      ####################################\n","\n","      generated_tokens.append(selected_index)\n","\n","      if neural_lm.vocab.idx2word()[selected_index] == \"<end>\":\n","        break\n","\n","    return \" \".join([neural_lm.vocab.idx2word()[i]\n","                     for i in input_tokens + generated_tokens])\n","\n","\n","print(\"k=1\")\n","print(generate_text_neural(neural_lm, \"lawyers think the\", k=1, max_tokens=10))\n","\n","print(\"k=2000\")\n","for _ in range(5):\n","  print(generate_text_neural(neural_lm, \"lawyers think the\", k=2000, max_tokens=10))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":0}
