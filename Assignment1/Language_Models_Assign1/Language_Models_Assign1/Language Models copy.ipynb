{"cells":[{"cell_type":"markdown","metadata":{"id":"0Rv18UaRljJX"},"source":["## CPSC 436N Assignment 1 - Language Models\n","\n","In this assignment you will implement both a count-based N-gram language model and a simple neural language model.\n","\n","For the neural model, familiarize yourself with pytorch: https://pytorch.org/tutorials/beginner/basics/intro.html\n"]},{"cell_type":"markdown","metadata":{"id":"h4otuXYsljJb"},"source":["\n","\n","Let's load a number of dependencies."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"7t9hPaLcljJd"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","from itertools import count\n","from scipy.special import softmax\n","from torch.nn.utils import clip_grad_norm_\n","from collections import defaultdict, Counter\n","\n","# Check if GPU is available to pytorch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"vhWxwA6JljJf"},"source":["Next, we implement the corpus and vocabulary objects. The vocabulary object has an index for each word type, and the corpus object reads a text corpus and returns the word type index for each word instance in the corpus. It is a list of lines.  "]},{"cell_type":"code","execution_count":27,"metadata":{"id":"--IW-bTprKHJ"},"outputs":[],"source":["class Vocabulary(object):\n","    def __init__(self):\n","        self.word2idx = defaultdict(count(0).__next__)\n","\n","    def add_word(self, word):\n","        _ = self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","    def idx2word(self):\n","        return {i: w for w, i in self.word2idx.items()}\n","\n","\n","class Corpus(object):\n","    def __init__(self, path, n=2, vocab=None):\n","        # Only initialize for the train corpus.\n","        # Then, for the dev and test corpus, use the vocabulary\n","        # from the training set\n","        if vocab is None:\n","          self.vocab = Vocabulary()\n","        else:\n","          self.vocab = vocab\n","\n","        # Read the entire corpus to memory\n","        lines = open(path).readlines()\n","\n","        # \"Tokenize\" (split to words) and add the <start> tokens based on the\n","        # number of n-grams, and the <end> tokens, for each line.\n","        lines = [['<start>'] * (n-1) + l.split() + ['<end>'] for l in lines]\n","\n","        # Convert word instance to word type ID\n","        lines = [[self.vocab.word2idx[w] for w in l] for l in lines]\n","\n","        self.lines = lines"]},{"cell_type":"markdown","metadata":{"id":"bMz414GKS2hO"},"source":["Load the Penn Tree Bank train, dev, and test sets."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":484,"status":"ok","timestamp":1689635643846,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"wmZBKmqjljJl","outputId":"6c7f675e-2c79-4a6e-fa84-9453424fe991"},"outputs":[{"name":"stdout","output_type":"stream","text":["train number of sentences: 25602, vocab size: 9473\n","dev number of sentences: 5120, vocab size: 9473\n","test number of sentences: 3413, vocab size: 9473\n"]}],"source":["train = Corpus(\"data/train.txt\")\n","dev = Corpus(\"data/dev.txt\", vocab=train.vocab)\n","test = Corpus(\"data/test.txt\", vocab=train.vocab)\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  print(f\"{name} number of sentences: {len(s.lines)}, vocab size: {len(s.vocab)}\")"]},{"cell_type":"markdown","metadata":{"id":"aSgcuuCAljJm"},"source":["### Bigram (Count-based) Language Model\n","\n","We will create a `CountBasedLM` object. Its training includes computing all the ngram counts in the corpus, for `n` and `n-1`. For inference you will need to complete the implementation of `compute_mle`."]},{"cell_type":"code","execution_count":62,"metadata":{"id":"iuKg7_wqljJm"},"outputs":[],"source":["def get_ngrams(s, n):\n","  \"\"\"\n","  Gets a list of words and returns a list of n-grams\n","  \"\"\"\n","  return zip(*[s[i:] for i in range(n)])\n","\n","\n","class CountBasedLM(object):\n","    def __init__(self, n=2, laplace=1):\n","        self.vocab = []\n","        self.vocab_size = len(self.vocab)\n","        self.laplace = laplace\n","        self.n = n\n","\n","    def train(self, train_corpus):\n","        self.vocab = train_corpus.vocab\n","        self.vocab_size = len(self.vocab)\n","\n","        # Count the n-grams in the corpus\n","        words = [w for line in train_corpus.lines for w in line]\n","\n","        self.n_gram_count = dict(Counter(get_ngrams(words, self.n)))\n","        self.nm1_gram_count = dict(Counter(get_ngrams(words, self.n-1)))\n","\n","    def compute_mle(self, n_gram):\n","        \"\"\"\n","        Compute the MLE of P(w_i|w_{i-n+1}...w_{i−1}),\n","        with add-one Laplacian smoothing.\n","        Please see chapter 3.5.1 of J&M 3rd Ed. for more information.\n","        \"\"\"\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        # P(w_i|w_{i-1}) = Count(w_i, w{i-1}) / Count(w_{i-1})\n","        if n_gram not in self.n_gram_count.keys():\n","            num, denom = 1, self.nm1_gram_count.get(n_gram[1:],0 ) + self.vocab_size\n","        else:\n","            num, denom= self.n_gram_count[n_gram], self.nm1_gram_count[n_gram[1:]]\n","        # As per intstruction, np.log() is applied to the probability\n","        # print(num,denom, round(num/denom, 6), round(np.log(num/denom), 6))\n","        prob = np.log(num / denom)\n","        ####################################\n","\n","        return prob"]},{"cell_type":"markdown","metadata":{"id":"lixRBxehljJy"},"source":["Let's create and train the bigram LM."]},{"cell_type":"code","execution_count":63,"metadata":{"id":"8YFIiHH-ljJy"},"outputs":[],"source":["bigram_lm = CountBasedLM()\n","bigram_lm.train(train)"]},{"cell_type":"markdown","metadata":{"id":"7-NrVBqzpRyh"},"source":["Now, you will implement a function that uses the LM to compute the probability of a sentence based on the chain rule. There are several important implementation details:\n","\n","1. The function gets a (string) sentence so the first step should be to tokenize it and convert the tokens to token IDs.\n","\n","2. Work in log space to avoid underflow, but return the probability (not log probability).\n","\n","We will test this function by making sure that:\n","\n","* The probability of each sentence is between 0 and 1\n","\n","* The probability of a sentence is not higher when a word is added\n","\n","* The probability of a grammatical sentence is higher than that of an ungrammatical sentence."]},{"cell_type":"code","execution_count":64,"metadata":{"id":"2ISvoVuwpRi5"},"outputs":[],"source":["def compute_probability(lm, sentence):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  # Tokenize and add <start> and <end> tokens. \n","  # Note the usage of the lower() function. This is make sure the n_grams \n","  # match the keys in the vocab.\n","  tokens = ['<start>'] * (lm.n-1) + sentence.lower().split() + ['<end>'] \n","  token_ids = [lm.vocab.word2idx[t] for t in tokens]\n","  # Get n_grams of the sentence\n","  n_grams = get_ngrams(token_ids, lm.n)\n","  probabilities = np.array([lm.compute_mle(n_gram) for n_gram in n_grams])\n","\n","  # take log, sum, and exp to get the probability\n","  probability = np.exp(np.sum(probabilities))\n","  # print(probabilities, probability)\n","  ####################################\n","  return probability\n","\n","\n","s1 = \"This is a sentence\"\n","prob_s1 = compute_probability(bigram_lm, s1)\n","assert(0 <= prob_s1 <= 1)\n","\n","s2 = \"This is a longer sentence\"\n","prob_s2 = compute_probability(bigram_lm, s2)\n","assert(prob_s2 <= prob_s1)\n","\n","s3 = \"This longer is a sentence\"\n","prob_s3 = compute_probability(bigram_lm, s3)\n","assert(prob_s3 < prob_s2)"]},{"cell_type":"markdown","metadata":{"id":"ZUZcxvwuVSJC"},"source":["Now, complete the code for computing average perplexity on a dataset using the LM. We will then compute the perplexity on each of the training, dev, and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689635644895,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"hmsntk-6VYBV","outputId":"c67a4515-82f7-48b6-df04-488c690f7151"},"outputs":[{"name":"stdout","output_type":"stream","text":["The average train perplexity for the bigram LM: 73.67764504117923\n","The average dev perplexity for the bigram LM: 92.5652414291731\n","The average test perplexity for the bigram LM: 88.3328433862738\n"]}],"source":["def compute_perplexity(lm, corpus):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","  \n","  ####################################\n","  return perplexity\n","\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  ppl = compute_perplexity(bigram_lm, s)\n","  print(f\"The average {name} perplexity for the bigram LM: {ppl}\")"]},{"cell_type":"markdown","metadata":{"id":"mVXSQzPDwXTq"},"source":["Finally, let's train a trigram model and compare their perplexities on the dev set (remember, lower perplexity is better)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":405,"status":"ok","timestamp":1689635645298,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"kSUjRKSOwXCO","outputId":"bbb8b519-461f-457b-ba97-ddd1a799b154"},"outputs":[{"name":"stdout","output_type":"stream","text":["The average train perplexity for the bigram LM: 155.0793650719732\n","The average dev perplexity for the bigram LM: 221.4983370622599\n","The average test perplexity for the bigram LM: 215.23674252950858\n"]}],"source":["trigram_lm = CountBasedLM(n=3)\n","trigram_lm.train(train)\n","\n","for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n","  ppl = compute_perplexity(trigram_lm, s)\n","  print(f\"The average {name} perplexity for the bigram LM: {ppl}\")"]},{"cell_type":"markdown","metadata":{"id":"ZGuzE_TXVYPO"},"source":["We can now use the count-based LM to generate text. This is done by starting with an input `prompt`, computing the distribution for the next token, and sampling from it. We will implement `top k` decoding that prunes the distribution to the k most probable next tokens, re-normalizes it and samples from it proportionally to the probability for each token. Using `k=1` would enable greedy decoding, i.e. selecting the most probable next token at each time step. We will generate the text once with greedy decoding and 5 times with `k=1000`. Complete the following code."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84285,"status":"ok","timestamp":1689637850491,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"n95DYxr9GBDb","outputId":"9fcc9c05-7dd2-4a9c-bda3-662482046fb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["k=1\n","<start> lawyers think the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n","k=1000\n","<start> lawyers think the soviet union rejected a year later the fabric of $\n","<start> lawyers think the anc 's certainly the counter to reduce domestic sales <end>\n","<start> lawyers think the dow jones of N N billion <end>\n","<start> lawyers think the pilots pursue <unk> road with $ N of the terms\n","<start> lawyers think the economic support to review board options research inc. a steep\n"]}],"source":["def generate_text_count_based(lm, prompt, k=1, max_tokens=10):\n","    # Tokenize and convert the sentence to a list of IDs\n","    tokens = ['<start>'] * (lm.n-1) + prompt.split()\n","    input_tokens = [lm.vocab.word2idx[t] for t in tokens]\n","\n","    # Iteratively generate the next word until generating <end>\n","    # or until reaching max_tokens.\n","    generated_tokens = []\n","\n","    for i in range(max_tokens):\n","      ####################################\n","      #   Your code here\n","      ####################################\n","\n","      ####################################\n","\n","      generated_tokens.append(selected_index)\n","\n","      if lm.vocab.idx2word()[selected_index] == \"<end>\":\n","        break\n","\n","    return \" \".join([lm.vocab.idx2word()[i] for i in input_tokens + generated_tokens])\n","\n","\n","print(\"k=1\")\n","print(generate_text_count_based(bigram_lm, \"lawyers think the\", k=1, max_tokens=10))\n","\n","print(\"k=2000\")\n","for _ in range(5):\n","  print(generate_text_count_based(bigram_lm, \"lawyers think the\", k=2000, max_tokens=10))"]},{"cell_type":"markdown","metadata":{"id":"p10QIUn9qasE"},"source":["### Neural Language Model\n","\n","The neural LM is an Ngram LM with one hidden layer as we learned in class. Let's start by defining some hyperparameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99hAuwZaqasE"},"outputs":[],"source":["embed_size = 128\n","hidden_size = 1024\n","num_epochs = 2\n","learning_rate = 2e-3"]},{"cell_type":"markdown","metadata":{"id":"8cl7DhAIljJ0"},"source":["Now, you will implement the class for the neural ngram language model. You will complete the `forward` function that gets a tensor with the context (n-1) token IDs, and returns the unnormalized next token probability distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZi51vWzljJ1"},"outputs":[],"source":["class NeuralNgramLM(nn.Module):\n","    def __init__(self, vocab, embed_size, intermediate_size, n=2):\n","        super(NeuralNgramLM, self).__init__()\n","        self.vocab = vocab\n","        self.vocab_size = len(self.vocab)\n","        self.n = n\n","        self.embed = nn.Embedding(self.vocab_size, embed_size)\n","        self.hidden = nn.Linear((self.n-1) * embed_size, hidden_size)\n","        self.output = nn.Linear(hidden_size, self.vocab_size)\n","\n","    def forward(self, x):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"iFfNEeZrljJ1"},"source":["Next, let's create and train a neural bigram model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gK_WyjiOljJ1"},"outputs":[],"source":["neural_lm = NeuralNgramLM(train.vocab, embed_size, hidden_size).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(\n","    neural_lm.parameters(), lr=learning_rate, momentum=0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71077,"status":"ok","timestamp":1689635724026,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"jb-yeBUKljJ2","outputId":"6e007d5b-54e9-495f-ade0-0f72f61bcdea"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 401/401 [00:34<00:00, 11.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch [1/2], Training Loss: 0.0206, Dev Perplexity: 22.77\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 401/401 [00:32<00:00, 12.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch [2/2], Training Loss: 0.0173, Dev Perplexity: 19.04\n","\n"]}],"source":["# Reduce batch size if you are running out of memory\n","batch_size = 64\n","\n","# Lets convert the corpora into tensors.\n","# For the training data, we will need to pad all the sentences\n","# to be in the same length because we are working in batches.\n","# But we don't need the same for the dev set.\n","pad_token = train.vocab.word2idx[\"<end>\"]\n","max_len = max([len(line) for line in train.lines])\n","padded = [line + (max_len - len(line) + 1) * [pad_token] for line in train.lines]\n","train_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in padded]\n","training_data = torch.cat(train_ids, dim=0).to(device)\n","\n","dev_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in dev.lines]\n","\n","neural_lm.train()\n","\n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for i in tqdm(range(0, len(training_data), batch_size)):\n","        batch = training_data[i:i + batch_size]\n","\n","        # Notice that we get a [batch_size, max_len-1] tensor\n","        # which would only work for a bigram LM.\n","        # If we want to train a n > 2 LM, we would need\n","        # add a dimension for the context.\n","        inputs = batch[:, :-1].to(device)\n","        targets = batch[:,1:].to(device)\n","\n","        # Forward pass\n","        # (the outputs shape should be [batch_size, max_len, hidden_size])\n","        outputs = neural_lm(inputs)\n","        loss = criterion(\n","            outputs.reshape(-1, neural_lm.vocab_size), targets.reshape(-1))\n","        total_loss += loss.item()\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Calculate the perplexity of the current trained model on\n","    # dev set. It should reduce between epochs. We will not\n","    # use our implementation of perplexity but instead\n","    # we will use cross-entropy loss. See J&M chapter 3.8\n","    # for the relationship between the two.\n","    total_ppl = 0\n","    for i in range(0, len(dev_ids)):\n","        dev_inputs = dev_ids[i][:, :-1].to(device)\n","        dev_targets = dev_ids[i][:,1:].to(device)\n","        dev_outputs = neural_lm(dev_inputs)\n","        ce = criterion(\n","            dev_outputs.reshape(-1, neural_lm.vocab_size),\n","            dev_targets.reshape(-1))\n","        total_ppl += ce.item();\n","\n","    print(f\"\\nEpoch [{epoch + 1}/{num_epochs}], \" + \\\n","          f\"Training Loss: {total_loss/len(train_ids):.4f}, \" + \\\n","          f\"Dev Perplexity: {2**(total_ppl/len(dev_ids)):5.2f}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"sCbcvvDwljJ2"},"source":["Save the trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEU68Mc0ljJ3"},"outputs":[],"source":["torch.save(neural_lm, f\"neural_{neural_lm.n}gram_lm.ckpt\")"]},{"cell_type":"markdown","metadata":{"id":"TRvZAdgSljJ3"},"source":["Finally, let's load the model and test it on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1647,"status":"ok","timestamp":1689635725658,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"UKu9d0lLljJ3","outputId":"7fecb390-6cca-485e-8fa7-f4007a4b0f9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Perplexity: 18.71\n","\n"]}],"source":["test_ids = [torch.from_numpy(np.array(line)).unsqueeze(0) for line in test.lines]\n","\n","neural_model = torch.load(f\"neural_{neural_lm.n}gram_lm.ckpt\")\n","neural_model.eval()\n","test_ppl = 0\n","with torch.no_grad():\n","    for i in range(0, len(test_ids)):\n","      inputs = test_ids[i][:, :-1].to(device)\n","      targets = test_ids[i][:,1:].to(device)\n","      outputs = neural_lm(inputs)\n","      ce = criterion(outputs.reshape(-1, neural_lm.vocab_size),\n","                     targets.reshape(-1))\n","      test_ppl += ce.item()\n","\n","print(f\"Test Perplexity: {2**(test_ppl/len(test_ids)):5.2f}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"WIQEXb6vTyul"},"source":["We can compute the probability of sentences using the neural LM. In fact, cross entropy loss between the first `|S|-1` tokens and the `|S|-1` last tokens is the negative log likelihood of a sequence, i.e. it is -1 * the average/sum of log probabilities for each token. Thanks to the monotonicity of log function, lower cross entropy loss corresponds to higer probability for a sentence. Since we are typically interested in comparing probabilities of sentences (rather than the absolute value), we can use CE loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhNuNs-IT8Pr"},"outputs":[],"source":["def compute_negative_log_likelihood(neural_lm, sentence):\n","  ####################################\n","  #   Your code here\n","  ####################################\n","\n","  ####################################\n","\n","s1 = \"This is a sentence\"\n","prob_s1 = -compute_negative_log_likelihood(neural_lm, s1)\n","\n","s2 = \"This is sentence a\"\n","prob_s2 = -compute_negative_log_likelihood(neural_lm, s2)\n","assert(prob_s1 >= prob_s2)"]},{"cell_type":"markdown","metadata":{"id":"gnyiaeVc3SS9"},"source":["We can also use the neural LM to generate text. Complete the following code."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97659,"status":"ok","timestamp":1689638116004,"user":{"displayName":"Vered Shwartz","userId":"17262788620607191056"},"user_tz":420},"id":"ukTvd6oV5bAh","outputId":"911a4cdf-9897-467b-803a-889709e25c05"},"outputs":[{"name":"stdout","output_type":"stream","text":["k=1\n","<start> lawyers think the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n","k=2000\n","<start> lawyers think the behaved to continuing deterioration identity ray encouraging bang newark approves\n","<start> lawyers think the require services dallas hancock wary topped mail surveyed industries press\n","<start> lawyers think the fha important greene and attributes provider collaboration norway judiciary marks\n","<start> lawyers think the turbulence widened he population decline earmarked buddy shipbuilding rod the\n","<start> lawyers think the fame the praised welcome providing anti-abortionists co secrets at&t filters\n"]}],"source":["def generate_text_neural(neural_lm, prompt, k=1, max_tokens=10):\n","    neural_lm.eval()\n","\n","    # Tokenize and convert the sentence to a list of IDs\n","    tokens = ['<start>'] * (neural_lm.n-1) + prompt.split()\n","    input_tokens = [neural_lm.vocab.word2idx[t] for t in tokens]\n","\n","    # Iteratively generate the next word until generating <end>\n","    # or until reaching max_tokens.\n","    generated_tokens = []\n","\n","    for i in range(max_tokens):\n","      token_ids = torch.tensor(input_tokens + generated_tokens, device=device)\n","\n","      ####################################\n","      #   Your code here\n","      ####################################\n","\n","      ####################################\n","\n","      generated_tokens.append(selected_index)\n","\n","      if neural_lm.vocab.idx2word()[selected_index] == \"<end>\":\n","        break\n","\n","    return \" \".join([neural_lm.vocab.idx2word()[i]\n","                     for i in input_tokens + generated_tokens])\n","\n","\n","print(\"k=1\")\n","print(generate_text_neural(neural_lm, \"lawyers think the\", k=1, max_tokens=10))\n","\n","print(\"k=2000\")\n","for _ in range(5):\n","  print(generate_text_neural(neural_lm, \"lawyers think the\", k=2000, max_tokens=10))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
