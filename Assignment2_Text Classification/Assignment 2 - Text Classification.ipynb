{"cells":[{"cell_type":"markdown","metadata":{"id":"c5842d2f"},"source":["# CPSC 43N Assignment 2 - Text Classification\n","\n","Follow the instructions in this notebook to develop 3 text classifiers:\n","\n","1. Bag-of-words (BOW) classifier.\n","2. Word embedding-based (CBOW) classifier.\n","3. Transformer-based classifier.\n","\n","You will test these classifiers on subsets of the 20 newsgroup dataset, and analyze the errors and successes of each model compared to the other.\n","\n"],"id":"c5842d2f"},{"cell_type":"markdown","metadata":{"id":"7b548499"},"source":["## Load Dataset\n","\n","For this assignment, we train and test classification models on the 20 newsgroups dataset. This dataset comprises around 18,000 newsgroups posts on 20 topics. It is split into 2 subsets (train and test) by `sklearn`.\n","\n","To ensure this assignment is manageable and won't take too long for training and inference, we will use the subset of 20 newsgroups only covering samples belonging to either one of the two classes ('__talk.politics.misc__' and '__talk.religion.misc__' used below). With this setting, we will perform a  binary classification instead of multiclass classification.\n","\n","Please **read carefully** the two links below which provide details about the 20newsgroups corpus and how to load and process it with sklearn:\n","\n","* http://qwone.com/~jason/20Newsgroups/\n","* https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"],"id":"7b548499"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dc4d1f2"},"outputs":[],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_train = fetch_20newsgroups(\n","    subset='train', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","newsgroups_test = fetch_20newsgroups(\n","    subset='test', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","# Remove empty documents\n","for s, name in zip([newsgroups_train, newsgroups_test], [\"train\", \"test\"]):\n","  empty_indices = {i for i, doc in enumerate(s.data) if len(doc) == 0}\n","  orig_len = len(s.data)\n","  for k in ['data', 'filenames', 'target', 'DESCR']:\n","    s[k] = [s[k][i] for i in range(orig_len) if i not in empty_indices]\n","  print(f\"Removed {len(empty_indices)} empty documents from the {name} set. Before: {orig_len}. After: {len(s.data)}.\")"],"id":"4dc4d1f2"},{"cell_type":"markdown","metadata":{"id":"al2PNNY1M94C"},"source":["Let's look at the data! Specifically, using the training set, let's find the length of the shortest and longest message in terms of number of characters, and the number of examples from each class."],"id":"al2PNNY1M94C"},{"cell_type":"code","execution_count":null,"metadata":{"id":"64W_1-AekQdp"},"outputs":[],"source":["####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Shortest message: {shortest} chars. Longest message: {longest} chars.\")\n","\n","# label_balance should be a dictionary from class name to the number of\n","# examples from that class in the train set\n","print(label_balance)"],"id":"64W_1-AekQdp"},{"cell_type":"markdown","metadata":{"id":"0eae9dd1"},"source":["## Part 1 - BOW Classifier\n","\n","The first classifier we will train is a bag-of-words (BOW) classifier, such as we learned in class:\n","\n","![](https://drive.google.com/uc?export=view&id=1HcgY3jHSuXoAMaBWmDnY4FlCoLghxh84)\n","\n","Training this classifier requires the following steps:\n","\n","1. Preprocessing the data, i.e. preparing the text features we would like to include. In our case, we will use the text content and will tokenize it.\n","\n","2. Vectorizing the data. Each instance will be represented as a high-dimensional vector indicating the count of each word in the vocabulary.\n","\n","3. Training the classifier.\n"],"id":"0eae9dd1"},{"cell_type":"markdown","source":["### Tokenization\n","\n","[Tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/) is a critical preprocessing step when we work with text data. A basic tokenizer separates the input text into tokens, which can be either words, characters, or subwords. In this assignment, we include additional processing to reduce the noise caused by typos and frequent but insignificant words contained in text."],"metadata":{"id":"npklSDPZBsxJ"},"id":"npklSDPZBsxJ"},{"cell_type":"markdown","metadata":{"id":"Igaf9WsjXeLu"},"source":["First, download and install the trained English pipeline ([en_core_web_lg](https://spacy.io/models/en) (https://spacy.io/models/en)) provided by Spacy:"],"id":"Igaf9WsjXeLu"},{"cell_type":"code","source":["import spacy\n","\n","!python -m spacy download en_core_web_lg"],"metadata":{"id":"qH6ssKguxSc6"},"id":"qH6ssKguxSc6","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, complete the code below to perform the following preprocessing:\n","\n","* Split the text into words\n","\n","* Lowercase the words\n","\n","* Remove stop words (which we expect to be less informative for text classification)\n","\n","* Remove punctuations\n","\n","* Lemmatize the tokens (to reduce the number of distinct words, or features, which would help the classifier generalize better)."],"metadata":{"id":"D1t2TX3CDCig"},"id":"D1t2TX3CDCig"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b47c6ae3"},"outputs":[],"source":["import string\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# Load trained English pipeline \"en_core_web_lg\"\n","nlp = spacy.load('en_core_web_lg')\n","\n","# Creating your own tokenizer function with functions built in Spacy.\n","def custom_tokenizer(doc):\n","\n","    ####################################\n","    #   Your code here\n","    ####################################\n","\n","    ####################################\n","\n","    # return preprocessed list of tokens (strings)\n","    return tokens"],"id":"b47c6ae3"},{"cell_type":"markdown","metadata":{"id":"5c1cdae7"},"source":["### Build the pipeline for the BOW classification model\n","\n","Now let's design the pipeline for the BOW classifier with sklearn. The overall pipeline for it should contain:\n","\n","1. A BOW vectorizer applying the tokenizer implemented above\n","\n","2. A classifier, which should be set to logistic regression for now\n","\n","To learn how to use `CountVectorizer` to obtain BOW vectors with a customized tokenizer:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","\n","To learn how to use the Pipeline object to implement classification models:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n","\n","\n"],"id":"5c1cdae7"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f08cf39"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","bow_vector = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 1))\n","classifier = LogisticRegression(max_iter=1000)\n","\n","# Create pipeline for BOW classfier.\n","pipe = Pipeline([('vectorizer', bow_vector), ('classifier', classifier)])"],"id":"7f08cf39"},{"cell_type":"markdown","metadata":{"id":"4805075b"},"source":["Now let's train the model on the training set obtained from 20 newsgroups."],"id":"4805075b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"UOCdNLWWcQIX"},"outputs":[],"source":["pipe.fit(newsgroups_train.data, newsgroups_train.target)"],"id":"UOCdNLWWcQIX"},{"cell_type":"markdown","metadata":{"id":"5szy3bROcYTF"},"source":["We can now evaluate the classifier's performance on the test set. As an example, here we only compute [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) as the evaluation metric. Other evaluation metrics such as [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), and [F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), may be used for deeper model analysis."],"id":"5szy3bROcYTF"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f38e6d29"},"outputs":[],"source":["from sklearn import metrics\n","\n","predicted = pipe.predict(newsgroups_test.data)\n","\n","# Model Accuracy\n","print(f\"Logistic Regression Accuracy: {metrics.accuracy_score(newsgroups_test.target, predicted)*100:.2f}%\")"],"id":"f38e6d29"},{"cell_type":"markdown","source":["You may want to print the number of features with `classifier.coef_.shape[-1]`. Since we trained a BOW model, the number of features should be equal to the vocabulary size after preprocessing."],"metadata":{"id":"4QQ6Gkp3wCfZ"},"id":"4QQ6Gkp3wCfZ"},{"cell_type":"markdown","metadata":{"id":"RtGM3zdNU13g"},"source":["### Model Variations\n","\n","Train and test the following variations: (1) changing the logistic regression classifier to Naive Bayes; and (2) including unigram __and bigram__ features. You may either make one modification at a time or experiment with combinations of these design choices. Implement the solution in the code cell below and print the accuracies of the different classifiers."],"id":"RtGM3zdNU13g"},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","\n","####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Naive Bayes Accuracy: {metrics.accuracy_score(newsgroups_test.target, nb_predicted)*100:.2f}%\")\n","\n","\n","####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Bigram LR Accuracy: {metrics.accuracy_score(newsgroups_test.target, bigram_predicted)*100:.2f}%\")\n","####################################"],"metadata":{"id":"cf3znzNuFnWJ"},"id":"cf3znzNuFnWJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"799bc5d3"},"source":["## Part 2 - Word Embedding-based classifier\n","\n","The pipeline of BOW classifier implemeted above consists of two components: BOW vectorizer and the classifier (LR or NB). In that scenario, in sklearn our customized tokenizer could be called together with the BOW vectorizer.\n","\n","This new classifier will use a distributed representation of the input document as the average of the embeddings of the words contained in the document. This is called CBOW, since it is the continous (and low-dimensional) version of the BOW approach, in which we summed the one-hot vectors of the words in the document.\n","\n","We could potentially re-use the custom tokenizer as is and replace only the vectorizer. However, to make use of Spacy word embeddings, that are part of the annotation process, we will implement a combined tokenizer and vectorizer that gets a document and returns its feature vector (i.e., average of word embeddings in the document). So for the sake of simplicity, we will not remove punctuation and stop words, nor lemmatize the words.  \n","\n","Complete the code in the following function in the cell below:\n","\n","* __transform( )__: gets `X`,  containing all the documents in the input set (i.e. train or test set), and converts each document into the average of its word embeddings, returning a list of feature vectors. This will be called for both the train and test data.\n","\n","We include the following function as it's a part of the `BaseEstimator` class:\n","\n","* __fit( )__: learns the model parameters from the training data. For the vectorizer, it doesn't do anything."],"id":"799bc5d3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cedf2f46"},"outputs":[],"source":["import numpy as np\n","from sklearn.base import BaseEstimator\n","\n","\n","class CBOWVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.dim = 300\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","# Create the pipeline for the word embedding-based classfier, and train it.\n","cbow_classifier = LogisticRegression(max_iter=1000)\n","cbow_pipe = Pipeline([\n","    (\"vectorizer\", CBOWVectorizer(nlp)), (\"classifier\", cbow_classifier)])\n","cbow_pipe.fit(newsgroups_train.data, newsgroups_train.target)"],"id":"cedf2f46"},{"cell_type":"markdown","metadata":{"id":"Muo8_fphuMba"},"source":["Let's evaluate the CBOW classifier on the test set to compare it with the best BOW performance."],"id":"Muo8_fphuMba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2f9c011"},"outputs":[],"source":["cbow_predicted = cbow_pipe.predict(newsgroups_test.data)\n","print(f\"CBOW Accuracy: {metrics.accuracy_score(newsgroups_test.target, cbow_predicted)*100:.2f}\")"],"id":"e2f9c011"},{"cell_type":"markdown","source":["## Part 3 - Transformer-Based classifier\n","\n","Finally, in the last part of the assignment, you will develop a text classifier based on a pretrained language model, RoBERTa.\n","\n","In the previous part, we encoded each word separately into a static word embedding. In this part, we encode the entire document with a contextualized representation, which allows to dynamically compute word representations that represent the appropriate sense of the word in the given context.\n","\n","To represent the entire document (__pooling__), we will take the embedding of the `[CLS]` token, or the first embedding.    \n","\n","This is an overview of the classifier:\n","\n","![](https://drive.google.com/uc?export=view&id=1HcsfSZ4Rix7-je4JWJqM_u9majur7Vw6)\n","\n","Note that we will not fine-tune RoBERTa but instead use the vectors from RoBERTa as feature vectors (updating only the classifier parameters)."],"metadata":{"id":"_VPPW4x8Yazw"},"id":"_VPPW4x8Yazw"},{"cell_type":"markdown","source":["First, download and install another Spacy English pipeline, [en_core_web_trf](https://spacy.io/models/en). This is a transformer pipeline based on [RoBERTa-base](https://ai.meta.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/).\n","\n"],"metadata":{"id":"MPkeH36KY3F3"},"id":"MPkeH36KY3F3"},{"cell_type":"code","source":["!pip install spacy-transformers\n","!python -m spacy download en_core_web_trf\n","\n","import spacy_transformers"],"metadata":{"id":"8uTTFLvEY2G-"},"id":"8uTTFLvEY2G-","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp_trf = spacy.load('en_core_web_trf')"],"metadata":{"id":"YVl83n_pksH4"},"id":"YVl83n_pksH4","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, you are required to implement a new vectorizer that takes the text documents and returns a RoBERTa-based vector representation for each document. Please check the [Spacy documentation](https://spacy.io/api/transformer) to understand how to obtain RoBERTa embeddings and the [Transformers documentation](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) to understand the outputs and dimensions. Please add a comment to explain your code."],"metadata":{"id":"nNy3h0yobdIA"},"id":"nNy3h0yobdIA"},{"cell_type":"code","source":["import cupy as cp\n","\n","\n","class RobertaVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.dim = 768\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","# Create the pipeline for the word embedding-based classfier, and train it.\n","trf_classifier = LogisticRegression(max_iter=1000)\n","trf_pipe = Pipeline([\n","    (\"vectorizer\", RobertaVectorizer(nlp_trf)), (\"classifier\", trf_classifier)])"],"metadata":{"id":"qu-WlwwZbdVz"},"id":"qu-WlwwZbdVz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are now ready to train this classifier. Please read: https://spacy.io/usage/embeddings-transformers for how to use GPU for model training and inference."],"metadata":{"id":"BvYgMJrdg3Rk"},"id":"BvYgMJrdg3Rk"},{"cell_type":"code","source":["from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu\n","\n","# Use GPU if available\n","if prefer_gpu():\n","  set_gpu_allocator(\"pytorch\")\n","  require_gpu(0)\n","  print(\"Using GPU.\")\n","else:\n","  print(\"GPU unavailable.\")\n","\n","# Model Training (This may take > 10 min depending on the GPU)\n","trf_pipe.fit(newsgroups_train.data, newsgroups_train.target)"],"metadata":{"id":"O4dGI2_xg9ni"},"id":"O4dGI2_xg9ni","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, let's evaluate the transformer-based classifier."],"metadata":{"id":"tt6XlCVAsj8J"},"id":"tt6XlCVAsj8J"},{"cell_type":"code","source":["trf_predicted = trf_pipe.predict(newsgroups_test.data)\n","print(f\"Transformer Accuracy: {metrics.accuracy_score(newsgroups_test.target, trf_predicted)*100:.2f}\")"],"metadata":{"id":"fnvflkM2sn9d"},"id":"fnvflkM2sn9d","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}