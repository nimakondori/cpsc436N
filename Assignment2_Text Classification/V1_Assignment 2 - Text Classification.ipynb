{"cells":[{"cell_type":"markdown","id":"c5842d2f","metadata":{"id":"c5842d2f"},"source":["# CPSC 43N Assignment 2 - Text Classification\n","\n","Follow the instructions in this notebook to develop 3 text classifiers:\n","\n","1. Bag-of-words (BOW) classifier.\n","2. Word embedding-based (CBOW) classifier.\n","3. Transformer-based classifier.\n","\n","You will test these classifiers on subsets of the 20 newsgroup dataset, and analyze the errors and successes of each model compared to the other.\n","\n"]},{"cell_type":"markdown","id":"7b548499","metadata":{"id":"7b548499"},"source":["## Load Dataset\n","\n","For this assignment, we train and test classification models on the 20 newsgroups dataset. This dataset comprises around 18,000 newsgroups posts on 20 topics. It is split into 2 subsets (train and test) by `sklearn`.\n","\n","To ensure this assignment is manageable and won't take too long for training and inference, we will use the subset of 20 newsgroups only covering samples belonging to either one of the two classes ('__talk.politics.misc__' and '__talk.religion.misc__' used below). With this setting, we will perform a  binary classification instead of multiclass classification.\n","\n","Please **read carefully** the two links below which provide details about the 20newsgroups corpus and how to load and process it with sklearn:\n","\n","* http://qwone.com/~jason/20Newsgroups/\n","* https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html"]},{"cell_type":"code","execution_count":1,"id":"4dc4d1f2","metadata":{"id":"4dc4d1f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed 21 empty documents from the train set. Before: 842. After: 821.\n","Removed 8 empty documents from the test set. Before: 561. After: 553.\n"]}],"source":["from sklearn.datasets import fetch_20newsgroups\n","\n","newsgroups_train = fetch_20newsgroups(\n","    subset='train', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","newsgroups_test = fetch_20newsgroups(\n","    subset='test', remove=('headers', 'footers', 'quotes'),\n","    categories=['talk.politics.misc','talk.religion.misc'])\n","\n","# Remove empty documents\n","for s, name in zip([newsgroups_train, newsgroups_test], [\"train\", \"test\"]):\n","  empty_indices = {i for i, doc in enumerate(s.data) if len(doc) == 0}\n","  orig_len = len(s.data)\n","  for k in ['data', 'filenames', 'target', 'DESCR']:\n","    s[k] = [s[k][i] for i in range(orig_len) if i not in empty_indices]\n","  print(f\"Removed {len(empty_indices)} empty documents from the {name} set. Before: {orig_len}. After: {len(s.data)}.\")"]},{"cell_type":"markdown","id":"al2PNNY1M94C","metadata":{"id":"al2PNNY1M94C"},"source":["Let's look at the data! Specifically, using the training set, let's find the length of the shortest and longest message in terms of number of characters, and the number of examples from each class."]},{"cell_type":"code","execution_count":2,"id":"64W_1-AekQdp","metadata":{"id":"64W_1-AekQdp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shortest message: 1 chars. Longest message: 49094 chars.\n","{'talk.politics.misc': 458, 'talk.religion.misc': 363}\n"]}],"source":["####################################\n","#   Your code here\n","####################################\n","\n","longest = len(max(newsgroups_train['data'], key=len))\n","shortest = len(min(newsgroups_train['data'], key=len))\n","\n","\n","class_names = ['talk.politics.misc','talk.religion.misc']\n","label_balance = {}\n","for target in newsgroups_train['target']:\n","    label_balance[class_names[target]] = label_balance.get(class_names[target], 0) + 1\n","####################################\n","\n","print(f\"Shortest message: {shortest} chars. Longest message: {longest} chars.\")\n","\n","# label_balance should be a dictionary from class name to the number of\n","# examples from that class in the train set\n","print(label_balance)"]},{"cell_type":"markdown","id":"0eae9dd1","metadata":{"id":"0eae9dd1"},"source":["## Part 1 - BOW Classifier\n","\n","The first classifier we will train is a bag-of-words (BOW) classifier, such as we learned in class:\n","\n","![](https://drive.google.com/uc?export=view&id=1HcgY3jHSuXoAMaBWmDnY4FlCoLghxh84)\n","\n","Training this classifier requires the following steps:\n","\n","1. Preprocessing the data, i.e. preparing the text features we would like to include. In our case, we will use the text content and will tokenize it.\n","\n","2. Vectorizing the data. Each instance will be represented as a high-dimensional vector indicating the count of each word in the vocabulary.\n","\n","3. Training the classifier.\n"]},{"cell_type":"markdown","id":"npklSDPZBsxJ","metadata":{"id":"npklSDPZBsxJ"},"source":["### Tokenization\n","\n","[Tokenization](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/) is a critical preprocessing step when we work with text data. A basic tokenizer separates the input text into tokens, which can be either words, characters, or subwords. In this assignment, we include additional processing to reduce the noise caused by typos and frequent but insignificant words contained in text."]},{"cell_type":"markdown","id":"Igaf9WsjXeLu","metadata":{"id":"Igaf9WsjXeLu"},"source":["First, download and install the trained English pipeline ([en_core_web_lg](https://spacy.io/models/en) (https://spacy.io/models/en)) provided by Spacy:"]},{"cell_type":"code","execution_count":3,"id":"qH6ssKguxSc6","metadata":{"id":"qH6ssKguxSc6"},"outputs":[],"source":["import spacy\n","\n","#!python -m spacy download en_core_web_lg"]},{"cell_type":"markdown","id":"D1t2TX3CDCig","metadata":{"id":"D1t2TX3CDCig"},"source":["Now, complete the code below to perform the following preprocessing:\n","\n","* Split the text into words\n","\n","* Lowercase the words\n","\n","* Remove stop words (which we expect to be less informative for text classification)\n","\n","* Remove punctuations\n","\n","* Lemmatize the tokens (to reduce the number of distinct words, or features, which would help the classifier generalize better)."]},{"cell_type":"code","execution_count":4,"id":"b47c6ae3","metadata":{"id":"b47c6ae3"},"outputs":[],"source":["import string\n","from spacy.lang.en import English\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","# Load trained English pipeline \"en_core_web_lg\"\n","nlp = spacy.load('en_core_web_lg')\n","\n","# Creating your own tokenizer function with functions built in Spacy.\n","def custom_tokenizer(doc):\n","\n","    ####################################\n","    #   Your code here\n","    ####################################\n","    tokens = nlp(doc.lower())  # lower case and tokenize\n","    tokens = [token.lemma_ for token in tokens if token.text not in STOP_WORDS and not token.is_punct]  # remove stopwords and punctuations, and then lemmatize\n","    \n","    #### Note: we tried the method bellow which gave higher accuracy, \n","    #### but splitting based on \"space\" causes some bugs, \n","    #### like \"#That\" which should be 2 tokens of \"#\" and \"That\" (a stopword). \n","    #### so we used the uncommented method above to directly use nlp's tokenizer.\n","    # lowercase = str.lower(doc)  # lowercase the text\n","    # words = lowercase.split()  # split into words\n","\n","    # drop_stop_words = [word for word in words if word not in STOP_WORDS]  # remove stop words\n","    # drop_punctuation = [''.join(char for char in text if char not in string.punctuation) for text in drop_stop_words]  # remove punctuation\n","\n","    # lemmatize (word by word): it gave a bit better results for NB, but we didn't keep this \n","    # tokens = [token.lemma_ for word in drop_punctuation for token in nlp(word)]  \n","\n","    ####################################\n","\n","    # return preprocessed list of tokens (strings)\n","    return tokens"]},{"cell_type":"code","execution_count":5,"id":"69e4706a","metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["\"that\" in STOP_WORDS"]},{"cell_type":"code","execution_count":6,"id":"b464acc3","metadata":{},"outputs":[{"data":{"text/plain":["\"#That describes some straights -- and nearly all homosexual males.\\n\\nCan you provide any evidence that doesn't ahve massive selection\\neffects?\\n\\nNo, I thought not.\\n\\nJust slander on your part.\\n\""]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["newsgroups_train['data'][2]"]},{"cell_type":"code","execution_count":7,"id":"1aed2c57","metadata":{},"outputs":[{"data":{"text/plain":["['describe',\n"," 'straight',\n"," 'nearly',\n"," 'homosexual',\n"," 'male',\n"," '\\n\\n',\n"," 'provide',\n"," 'evidence',\n"," 'ahve',\n"," 'massive',\n"," 'selection',\n"," '\\n',\n"," 'effect',\n"," '\\n\\n',\n"," 'think',\n"," '\\n\\n',\n"," 'slander',\n"," '\\n']"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["custom_tokenizer(newsgroups_train['data'][2])"]},{"cell_type":"markdown","id":"5c1cdae7","metadata":{"id":"5c1cdae7"},"source":["### Build the pipeline for the BOW classification model\n","\n","Now let's design the pipeline for the BOW classifier with sklearn. The overall pipeline for it should contain:\n","\n","1. A BOW vectorizer applying the tokenizer implemented above\n","\n","2. A classifier, which should be set to logistic regression for now\n","\n","To learn how to use `CountVectorizer` to obtain BOW vectors with a customized tokenizer:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n","\n","To learn how to use the Pipeline object to implement classification models:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n","\n","\n"]},{"cell_type":"code","execution_count":8,"id":"7f08cf39","metadata":{"id":"7f08cf39"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","bow_vector = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 1))\n","classifier = LogisticRegression(max_iter=1000)\n","\n","# Create pipeline for BOW classfier.\n","pipe = Pipeline([('vectorizer', bow_vector), ('classifier', classifier)])"]},{"cell_type":"markdown","id":"4805075b","metadata":{"id":"4805075b"},"source":["Now let's train the model on the training set obtained from 20 newsgroups."]},{"cell_type":"code","execution_count":9,"id":"UOCdNLWWcQIX","metadata":{"id":"UOCdNLWWcQIX"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hooma\\anaconda3\\envs\\Py_AS_XAI\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 CountVectorizer(tokenizer=&lt;function custom_tokenizer at 0x000001E7C5EAFCA0&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 CountVectorizer(tokenizer=&lt;function custom_tokenizer at 0x000001E7C5EAFCA0&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function custom_tokenizer at 0x000001E7C5EAFCA0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"],"text/plain":["Pipeline(steps=[('vectorizer',\n","                 CountVectorizer(tokenizer=<function custom_tokenizer at 0x000001E7C5EAFCA0>)),\n","                ('classifier', LogisticRegression(max_iter=1000))])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["pipe.fit(newsgroups_train.data, newsgroups_train.target)"]},{"cell_type":"markdown","id":"5szy3bROcYTF","metadata":{"id":"5szy3bROcYTF"},"source":["We can now evaluate the classifier's performance on the test set. As an example, here we only compute [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) as the evaluation metric. Other evaluation metrics such as [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), [precision](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), and [F1](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), may be used for deeper model analysis."]},{"cell_type":"code","execution_count":10,"id":"2f8350c0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression Accuracy: 72.33%\n"]}],"source":["from sklearn import metrics\n","\n","predicted = pipe.predict(newsgroups_test.data)\n","\n","# Model Accuracy\n","print(f\"Logistic Regression Accuracy: {metrics.accuracy_score(newsgroups_test.target, predicted)*100:.2f}%\")"]},{"cell_type":"markdown","id":"4QQ6Gkp3wCfZ","metadata":{"id":"4QQ6Gkp3wCfZ"},"source":["You may want to print the number of features with `classifier.coef_.shape[-1]`. Since we trained a BOW model, the number of features should be equal to the vocabulary size after preprocessing."]},{"cell_type":"markdown","id":"RtGM3zdNU13g","metadata":{"id":"RtGM3zdNU13g"},"source":["### Model Variations\n","\n","Train and test the following variations: (1) changing the logistic regression classifier to Naive Bayes; and (2) including unigram __and bigram__ features. You may either make one modification at a time or experiment with combinations of these design choices. Implement the solution in the code cell below and print the accuracies of the different classifiers."]},{"cell_type":"code","execution_count":11,"id":"6ae1ce43","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hooma\\anaconda3\\envs\\Py_AS_XAI\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Naive Bayes Accuracy: 76.31%\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hooma\\anaconda3\\envs\\Py_AS_XAI\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Bigram LR Accuracy: 79.02%\n"]}],"source":["from sklearn.naive_bayes import MultinomialNB\n","\n","####################################\n","#   Your code here\n","####################################\n","classifier = MultinomialNB()\n","\n","# Create pipeline for BOW classfier.\n","nb_pipe = Pipeline([('vectorizer', bow_vector), ('classifier', classifier)])\n","nb_pipe.fit(newsgroups_train.data, newsgroups_train.target)\n","nb_predicted = nb_pipe.predict(newsgroups_test.data)\n","####################################\n","\n","print(f\"Naive Bayes Accuracy: {metrics.accuracy_score(newsgroups_test.target, nb_predicted)*100:.2f}%\")\n","\n","####################################\n","#   Your code here\n","####################################\n","bigram_classifier = MultinomialNB()\n","\n","# Create pipeline for BOW classfier.\n","bow_vector = CountVectorizer(tokenizer=custom_tokenizer, ngram_range=(1, 2))\n","bigram_pipe = Pipeline([('vectorizer', bow_vector), ('classifier', bigram_classifier)])\n","bigram_pipe.fit(newsgroups_train.data, newsgroups_train.target)\n","bigram_predicted = bigram_pipe.predict(newsgroups_test.data)\n","####################################\n","\n","print(f\"Bigram LR Accuracy: {metrics.accuracy_score(newsgroups_test.target, bigram_predicted)*100:.2f}%\")\n","####################################"]},{"cell_type":"code","execution_count":24,"id":"cf3znzNuFnWJ","metadata":{"id":"cf3znzNuFnWJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\hooma\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Naive Bayes Accuracy: 76.13%\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\hooma\\anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Bigram LR Accuracy: 80.29%\n"]}],"source":[]},{"cell_type":"markdown","id":"799bc5d3","metadata":{"id":"799bc5d3"},"source":["## Part 2 - Word Embedding-based classifier\n","\n","The pipeline of BOW classifier implemeted above consists of two components: BOW vectorizer and the classifier (LR or NB). In that scenario, in sklearn our customized tokenizer could be called together with the BOW vectorizer.\n","\n","This new classifier will use a distributed representation of the input document as the average of the embeddings of the words contained in the document. This is called CBOW, since it is the continous (and low-dimensional) version of the BOW approach, in which we summed the one-hot vectors of the words in the document.\n","\n","We could potentially re-use the custom tokenizer as is and replace only the vectorizer. However, to make use of Spacy word embeddings, that are part of the annotation process, we will implement a combined tokenizer and vectorizer that gets a document and returns its feature vector (i.e., average of word embeddings in the document). So for the sake of simplicity, we will not remove punctuation and stop words, nor lemmatize the words.  \n","\n","Complete the code in the following function in the cell below:\n","\n","* __transform( )__: gets `X`,  containing all the documents in the input set (i.e. train or test set), and converts each document into the average of its word embeddings, returning a list of feature vectors. This will be called for both the train and test data.\n","\n","We include the following function as it's a part of the `BaseEstimator` class:\n","\n","* __fit( )__: learns the model parameters from the training data. For the vectorizer, it doesn't do anything."]},{"cell_type":"code","execution_count":12,"id":"cedf2f46","metadata":{"id":"cedf2f46"},"outputs":[],"source":["import numpy as np\n","from sklearn.base import BaseEstimator\n","\n","\n","class CBOWVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.dim = 300\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        avg_embeddings = []\n","\n","        for doc in X:\n","            doc = self.nlp(doc)\n","\n","            total_emb = np.zeros(self.dim)\n","            count = 0\n","\n","            for word in doc:\n","                if word.has_vector:\n","                    total_emb += word.vector\n","                    count += 1\n","\n","            avg_emb = total_emb / count if count > 0 else np.zeros(self.dim)\n","            avg_embeddings.append(avg_emb)\n","\n","            #### Note: we also tried doc.vector directly, but the result was worse! \n","            # so probably this low-level averaging of embeddings is better.\n","\n","        return np.array(avg_embeddings)\n","        ####################################\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","# Create the pipeline for the word embedding-based classfier, and train it.\n","cbow_classifier = LogisticRegression(max_iter=1000)\n","cbow_pipe = Pipeline([\n","    (\"vectorizer\", CBOWVectorizer(nlp)), (\"classifier\", cbow_classifier)])\n","cbow_pipe.fit(newsgroups_train.data, newsgroups_train.target)"]},{"cell_type":"markdown","id":"Muo8_fphuMba","metadata":{"id":"Muo8_fphuMba"},"source":["Let's evaluate the CBOW classifier on the test set to compare it with the best BOW performance."]},{"cell_type":"code","execution_count":29,"id":"e2f9c011","metadata":{"id":"e2f9c011"},"outputs":[{"name":"stdout","output_type":"stream","text":["CBOW Accuracy: 78.66\n"]}],"source":["cbow_predicted = cbow_pipe.predict(newsgroups_test.data)\n","print(f\"CBOW Accuracy: {metrics.accuracy_score(newsgroups_test.target, cbow_predicted)*100:.2f}\")"]},{"cell_type":"markdown","id":"_VPPW4x8Yazw","metadata":{"id":"_VPPW4x8Yazw"},"source":["## Part 3 - Transformer-Based classifier\n","\n","Finally, in the last part of the assignment, you will develop a text classifier based on a pretrained language model, RoBERTa.\n","\n","In the previous part, we encoded each word separately into a static word embedding. In this part, we encode the entire document with a contextualized representation, which allows to dynamically compute word representations that represent the appropriate sense of the word in the given context.\n","\n","To represent the entire document (__pooling__), we will take the embedding of the `[CLS]` token, or the first embedding.    \n","\n","This is an overview of the classifier:\n","\n","![](https://drive.google.com/uc?export=view&id=1HcsfSZ4Rix7-je4JWJqM_u9majur7Vw6)\n","\n","Note that we will not fine-tune RoBERTa but instead use the vectors from RoBERTa as feature vectors (updating only the classifier parameters)."]},{"cell_type":"markdown","id":"MPkeH36KY3F3","metadata":{"id":"MPkeH36KY3F3"},"source":["First, download and install another Spacy English pipeline, [en_core_web_trf](https://spacy.io/models/en). This is a transformer pipeline based on [RoBERTa-base](https://ai.meta.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/).\n","\n"]},{"cell_type":"code","execution_count":13,"id":"8uTTFLvEY2G-","metadata":{"id":"8uTTFLvEY2G-"},"outputs":[],"source":["# !pip install spacy-transformers\n","# !python -m spacy download en_core_web_trf\n","\n","import spacy_transformers"]},{"cell_type":"code","execution_count":14,"id":"YVl83n_pksH4","metadata":{"id":"YVl83n_pksH4"},"outputs":[],"source":["nlp_trf = spacy.load('en_core_web_trf')"]},{"cell_type":"code","execution_count":22,"id":"2dd29db7","metadata":{},"outputs":[{"data":{"text/plain":["(1, 768)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["token = nlp_trf(newsgroups_train.data[2])\n","# token._.trf_data.tensors[0]#.model_output['last_hidden_state']\n","token._.trf_data.model_output['last_hidden_state'][0,0]#.shape\n","# token\n","token._.trf_data.model_output['pooler_output'].shape\n","# token._.trf_data#.last_hidden_layer_state.data[0] #.tensors[-1].shape#.model_output['pooler_output']"]},{"cell_type":"markdown","id":"nNy3h0yobdIA","metadata":{"id":"nNy3h0yobdIA"},"source":["Again, you are required to implement a new vectorizer that takes the text documents and returns a RoBERTa-based vector representation for each document. Please check the [Spacy documentation](https://spacy.io/api/transformer) to understand how to obtain RoBERTa embeddings and the [Transformers documentation](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) to understand the outputs and dimensions. Please add a comment to explain your code."]},{"cell_type":"code","execution_count":23,"id":"qu-WlwwZbdVz","metadata":{"id":"qu-WlwwZbdVz"},"outputs":[],"source":["#import cupy as cp\n","from tqdm import tqdm\n","\n","class RobertaVectorizer(BaseEstimator):\n","    def __init__(self, nlp):\n","        self.nlp = nlp\n","        self.dim = 768\n","\n","    def transform(self, X):\n","        ####################################\n","        #   Your code here\n","        ####################################\n","        X_embeddings = []\n","\n","        batch_size = 2  # Adjust the batch size as needed\n","        with tqdm(total=len(X)) as progress_bar:\n","            for trf_output in tqdm(self.nlp.pipe(X, batch_size=2)): # get the transformer output of the document (in batches)\n","                # cls_output = trf_output._.trf_data.model_output['last_hidden_state'][0,0]  # get the last hidden state, its first element (<CLS>)\n","                cls_output = trf_output._.trf_data.model_output['pooler_output'][0]  # get the last hidden state, its first element (<CLS>)\n","                # cls_output = trf_output._.trf_data.tensors[-1][0]  # get the (<CLS>) from pooler output which is the pooled of the last 4 hidden state layers\n","                X_embeddings.append(cls_output)\n","                progress_bar.update(1)\n","        return X_embeddings\n","\n","        # batch_size = 16  # Adjust the batch size as needed\n","        # for i in tqdm(range(0, len(X), batch_size)):\n","        #     batch = X[i:i + batch_size]\n","        #     docs = list(self.nlp.pipe(batch))\n","        #     embeddings = [doc._.trf_data.tensors[-1][0] for doc in docs]\n","        #     X_embeddings.extend(embeddings)\n","\n","        # return np.asarray(X_embeddings)\n","        ####################################\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","# Create the pipeline for the word embedding-based classfier, and train it.\n","trf_classifier = LogisticRegression(max_iter=1000)\n","trf_pipe = Pipeline([\n","    (\"vectorizer\", RobertaVectorizer(nlp_trf)), (\"classifier\", trf_classifier)])"]},{"cell_type":"markdown","id":"BvYgMJrdg3Rk","metadata":{"id":"BvYgMJrdg3Rk"},"source":["We are now ready to train this classifier. Please read: https://spacy.io/usage/embeddings-transformers for how to use GPU for model training and inference."]},{"cell_type":"code","execution_count":24,"id":"O4dGI2_xg9ni","metadata":{"id":"O4dGI2_xg9ni"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU unavailable.\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/821 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":[" 61%|██████▏   | 503/821 [07:42<03:00,  1.76it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n","821it [12:13,  1.12it/s] [12:13<00:00,  2.40it/s]\n","100%|██████████| 821/821 [12:13<00:00,  1.12it/s]\n"]},{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 RobertaVectorizer(nlp=&lt;spacy.lang.en.English object at 0x000002609F6BF400&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n","                 RobertaVectorizer(nlp=&lt;spacy.lang.en.English object at 0x000002609F6BF400&gt;)),\n","                (&#x27;classifier&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobertaVectorizer</label><div class=\"sk-toggleable__content\"><pre>RobertaVectorizer(nlp=&lt;spacy.lang.en.English object at 0x000002609F6BF400&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div>"],"text/plain":["Pipeline(steps=[('vectorizer',\n","                 RobertaVectorizer(nlp=<spacy.lang.en.English object at 0x000002609F6BF400>)),\n","                ('classifier', LogisticRegression(max_iter=1000))])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["from thinc.api import set_gpu_allocator, require_gpu, prefer_gpu\n","\n","# Use GPU if available\n","if prefer_gpu():\n","  set_gpu_allocator(\"pytorch\")\n","  require_gpu(0)\n","  print(\"Using GPU.\")\n","else:\n","  print(\"GPU unavailable.\")\n","\n","# Model Training (This may take > 10 min depending on the GPU)\n","trf_pipe.fit(newsgroups_train.data, newsgroups_train.target)"]},{"cell_type":"markdown","id":"tt6XlCVAsj8J","metadata":{"id":"tt6XlCVAsj8J"},"source":["Finally, let's evaluate the transformer-based classifier."]},{"cell_type":"code","execution_count":25,"id":"fnvflkM2sn9d","metadata":{"id":"fnvflkM2sn9d"},"outputs":[{"name":"stderr","output_type":"stream","text":["553it [07:38,  1.21it/s] [07:38<00:00,  5.11it/s]\n","100%|██████████| 553/553 [07:38<00:00,  1.21it/s]\n"]},{"ename":"NameError","evalue":"name 'metrics' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\hooma\\workspace\\Courses\\CPSC436N\\Assignment Github\\Assignment2_Text Classification\\Final_Assignment 2 - Text Classification.ipynb Cell 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hooma/workspace/Courses/CPSC436N/Assignment%20Github/Assignment2_Text%20Classification/Final_Assignment%202%20-%20Text%20Classification.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trf_predicted \u001b[39m=\u001b[39m trf_pipe\u001b[39m.\u001b[39mpredict(newsgroups_test\u001b[39m.\u001b[39mdata)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hooma/workspace/Courses/CPSC436N/Assignment%20Github/Assignment2_Text%20Classification/Final_Assignment%202%20-%20Text%20Classification.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTransformer Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmetrics\u001b[39m.\u001b[39maccuracy_score(newsgroups_test\u001b[39m.\u001b[39mtarget,\u001b[39m \u001b[39mtrf_predicted)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"]}],"source":["trf_predicted = trf_pipe.predict(newsgroups_test.data)\n","print(f\"Transformer Accuracy: {metrics.accuracy_score(newsgroups_test.target, trf_predicted)*100:.2f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}
