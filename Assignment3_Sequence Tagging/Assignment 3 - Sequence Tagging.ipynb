{"cells":[{"cell_type":"markdown","metadata":{"id":"ESA2kDcMmNkJ"},"source":["# Assignment 3 - Sequence Tagging\n","\n","In this assignment we are going to develop two models for sequence tagging (labeling). As we learned in class, in sequence tagging we have an input sequence X = x1 ... xn and the goal is to assign a label (tag) yi to each word xi in the sequence.\n","\n","We will develop (1) a Hidden Markov Model (HMM) and (2) an RNN-based sequence tagger. We will also implement decoding with the Viterbi algorithm, since as we saw in class, greedily assigning the most likely tag for each token can result in a suboptimal sequence of POS tags.\n","\n","![](https://drive.google.com/uc?export=view&id=1HdBAFpMMEXQxhdLJfs4eAPZNXv_47ngz)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9AD7kg1fkklb"},"source":["### Loading the Dataset\n","\n","If you're working in Colab, make sure you upload the file `corpus.txt` to the working directory.\n","\n","We will load and save the `corpus.txt` POS-annotated corpus in a format different from its origianl one. In the file, each word is followed by an underscore and a tag that represents the word's correct part of speech in that context. For instance:\n","\n","<br>\n","\n","*(1) There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n","\n","*(2) Tosco_NNP said_VBD it_PRP expects_VBZ BP_NNP to_TO shut_VB the_DT plant_NN ._.*\n","\n","<br>\n","\n","We will instead reformat and load the same info as (token, pos_tag) lists. For example:\n","\n","<br>\n","\n","[\n","  [(there, EX), (are, VBP), (also, RB), (plant, NN), (and, CC), (gift, NN), (shop, NNS), (., .)],\n","\n","  [(tosco, NNP), (said, VBD), (it, PRP), (expects, VBZ), (BP, NNP), (to, To), (shut, VB), (the, DT), (plant, NN), (., .)]\n","]\n","\n","<br>\n","\n","**Please note that we convert each token into its lower case.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmreO0EUmLm-"},"outputs":[],"source":["dataset = [[tuple(w.split('_')) for w in line.strip().split()]\n","           for line in open(\"corpus.txt\")]\n","\n","# Lowercase the words\n","dataset = [[(w.lower(), pos) for (w, pos) in sentence] for sentence in dataset]\n","print(dataset[0])"]},{"cell_type":"markdown","source":["## Part 1 - Count-based Sequence Tagging with Viterbi Algorithm\n","\n","In the first part of this assignment, we will build a hidden Markov nodel to predict the part of speech tags for the words in a given sentence, using the *Viterbi Algorithm* (see textbook sec 8.4.5). In class we learned about the *Forward Algorithm* and Viterbi is a slight variation of that.\n","\n","![](https://drive.google.com/uc?export=view&id=1Hf0LEE9cvJTbetDcdlzgordreff0u1av)"],"metadata":{"id":"Os7EtCwL231C"}},{"cell_type":"markdown","source":["We start by obtaining the set of states for the HHM (the POSs) and set of observations (the tokens)."],"metadata":{"id":"JHk1JrnzDCgc"}},{"cell_type":"code","source":["observations = sorted(list(set.union(*[set(list(zip(*line))[0]) for line in dataset])))\n","example = observations[:5] + [\"...\"] + observations[-5:]\n","print(f\"Number of observations: {len(observations)}. {example}\")\n","\n","states = sorted(list(set.union(*[set(list(zip(*line))[1]) for line in dataset])))\n","example = states[:5] + [\"...\"] + states[-5:]\n","print(f\"Number of states: {len(states)}. {example}\")"],"metadata":{"id":"HWtlNcbf9Ga4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5phXfOnGueR_"},"source":["We would need to deal with out-of-vocabulary tokens that may appear during inference time (e.g. on a held-out test set). We solve this by adding an unknown token `<unk>` to the vocabulary (i.e., set of observations). In order for this token to get emission probabilities, we would need to add it into the dataset. We do so by replacing every token that appeared less than 2 times in the dataset with the `<unk>` token. Complete the code below to implement this solution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oW-YJkjcrr7R"},"outputs":[],"source":["####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","# Print <unk> frequency\n","print(sum([1 if w == \"<unk>\" else 0 for instance in dataset for w, _ in instance]))"]},{"cell_type":"markdown","metadata":{"id":"HBqKKgjJzWCT"},"source":["We are now ready to compute the emission probabilities. The emission probability matrix can be formed as a dictionary where the key is the POS tag, and the value is another dictionary, from a token to a probability. We will use MLE, i.e. the probability is computed as relative frequency, and we will apply add-1 smoothing to avoid 0 probabilities. Complete the code below. Don't forget to add the unobserved words in the smoothed probability distribution for each POS!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GR8sPEVUxwHm"},"outputs":[],"source":["####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Emission probability of 'money' in noun: {emission_probability['NN']['money']}\")\n","print(f\"Emission probability of 'yellow' in verb: {emission_probability['VB']['yellow']}\")"]},{"cell_type":"markdown","metadata":{"id":"BrgHDEf23WOO"},"source":["Now, let's compute the transition probabilities. The transition matrix can be similarly formed as a dictionary, this time from a state (pos) to a dictionary of states to probabilities. Complete the code below to compute the MLE probabilities with Add-1 smoothing. Again, don't forget to include the probabilities from each state to each state (not only those that occurred in the data!)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRvHUphUz2G7"},"outputs":[],"source":["####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Transition probability from determiner to noun: {transition_probability['DT']['NN']}\")\n","print(f\"Transition probability from verb to adjective: {transition_probability['VB']['JJ']}\")"]},{"cell_type":"markdown","source":["Finally, we can similarly compute the start probabilities. This will be a dictionary from POS to a probability."],"metadata":{"id":"etNLMutYUFz9"}},{"cell_type":"code","source":["####################################\n","#   Your code here\n","####################################\n","\n","####################################\n","\n","print(f\"Start probability for determiner: {start_probability['DT']}\")\n","print(f\"Start probability for verb: {start_probability['VB']}\")"],"metadata":{"id":"nTtle8KIQE5D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're done estimating the probabilities! Now we can implement the decoding method, i.e. find the most probable sequence of POS tags for a given sentence (i.e. sequence of tokens). We will do this by implementing the Viterbi algorithm (see J&M section 8.4.5). Complete the following code."],"metadata":{"id":"hIM4Uyi6U2sX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjjacMrG34Hb"},"outputs":[],"source":["import numpy as np\n","\n","\n","def Viterbi(curr_observations, # a sequence of tokens to decode\n","            states, start_prob, trans_prob, em_prob):\n","\n","    T = len(curr_observations)\n","\n","    # Create a probability matrix, viterbi, to denote the probability\n","    # to be in state s in time step t\n","    viterbi = {s: [0] * T for s in states}\n","\n","    # Create the backpointer matrix that saves the previous state\n","    backpointer = {s: [0] * T for s in states}\n","\n","    # Initialization step: set the path probability to the start probability\n","    # of the first state * emission probability of the first token.\n","    for s in states:\n","      viterbi[s][0] = start_prob[s] * em_prob[s][curr_observations[0]]\n","\n","    # Recursion step: the probability is the maximum probability\n","    # of transitioning from s_other in time step t-1.\n","\n","    for t in range(1, T):\n","      for s in states:\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","        backpointer[s][t] = s_other\n","        viterbi[s][t] = viterbi[s_other][t-1] * trans_prob[s_other][s] * em_prob[s][curr_observations[t]]\n","\n","    # Termination step: find the most likely state at time step t and trace back\n","    # to find the path.\n","    s_end = states[np.argmax([viterbi[s][T-1] for s in states])]\n","    best_prob = viterbi[s_end][T-1]\n","\n","    best_path = [s_end]\n","    for t in range(T-1, 0, -1):\n","      best_path.append(backpointer[best_path[-1]][t])\n","\n","    best_path = best_path[::-1]\n","    return best_path, best_prob\n","\n","\n","print(Viterbi(\"I want to find this\".lower().split(),\n","              states, start_probability,\n","              transition_probability, emission_probability))"]},{"cell_type":"markdown","metadata":{"id":"SyG4lTR4D7Lg"},"source":["## Part 2 - LSTM-Based POS Tagger\n","\n","In the second part of this assignment, we will use the same corpus to train a bidirectional stacked LSTM-based neural sequence labeling model to predict the part of speech tags of unknown input sentences.\n","\n","The following figure shows an RNN-based sequence tagger:\n","\n","![](https://drive.google.com/uc?export=view&id=1Hf79ROO3UU8BuePuacjhskYJrIq4lvr_)\n","\n","The inputs at each time step are (possibly pre-trained) word embeddings corresponding to the input tokens. They go into the RNN (e.g. LSTM in our assignment)Â and the outputs from the RNN at each time step represent go into a softmax layer to produce the distribution over the POS tagset for the given input."]},{"cell_type":"markdown","metadata":{"id":"iFaEQAQgEIew"},"source":["First, let's import the required packages.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cti1kgGqD9a4"},"outputs":[],"source":["import math\n","import tqdm\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","from sklearn.metrics import f1_score\n","from torch.nn.utils import clip_grad_norm_"]},{"cell_type":"markdown","metadata":{"id":"lps3RqC3FAgA"},"source":["Since we are training a model, we should now split the dataset into train, dev, and test sets, with the common 80%-10%-10% ratio. We start by shuffling the dataset to avoid any possible ordering bias."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6Mdvr5WCfZt"},"outputs":[],"source":["import random\n","\n","random.shuffle(dataset)\n","\n","training_data = dataset[0:math.floor(len(dataset)*0.8)]\n","dev_data = dataset[math.floor(len(dataset)*0.8):math.floor(len(dataset)*0.9)]\n","test_data = dataset[math.floor(len(dataset)*0.9):]"]},{"cell_type":"markdown","metadata":{"id":"9WzNS6VagmvT"},"source":["Next, we need to map tokens to indices so that we can learn the embeddings for each token. Similarly, we need to construct a dictionary of POS tags to indices. We will use the `Vocabulary` class from assignment 1. We will also create a `Corpus` class but adapt it to sequence tagging.\n","\n","As in part 1, we also need to deal with **unknown tokens** in testing, for which we will adapt these classes. Complete the code below to replace each (w, p) in the data to their respective IDs, and replace words that appeared fewer than 2 times with the `<unk>` token. Each instance in `corpus.data` should return a tuple with two items: a list of word IDs and a list of tag IDs."]},{"cell_type":"code","source":["from itertools import count\n","from collections import defaultdict\n","\n","\n","class Vocabulary(object):\n","    def __init__(self, add_unk=False):\n","        self.word2idx = defaultdict(count(0).__next__)\n","        self.add_unk = add_unk\n","\n","        if add_unk:\n","          _ = self.word2idx[\"<unk>\"]\n","\n","    def add_word(self, word):\n","        _ = self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","\n","    def idx2word(self):\n","        return {i: w for w, i in self.word2idx.items()}\n","\n","\n","class Corpus(object):\n","    def __init__(self, data, vocab=None, tagset=None):\n","        # Only initialize for the train corpus.\n","        # Then, for the dev and test corpus, use the vocabulary\n","        # from the training set\n","        if vocab is None:\n","          self.vocab = Vocabulary(add_unk=True)\n","        else:\n","          self.vocab = vocab\n","\n","        if tagset is None:\n","          self.tagset = Vocabulary(add_unk=False)\n","        else:\n","          self.tagset = tagset\n","\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","        self.data = data"],"metadata":{"id":"03a2841MVwN1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's create the `Corpus` object for each of the train, dev, and test sets."],"metadata":{"id":"dr1Zw9r6Xwnt"}},{"cell_type":"code","source":["train = Corpus(training_data)\n","dev = Corpus(dev_data, vocab=train.vocab, tagset=train.tagset)\n","test = Corpus(test_data, vocab=train.vocab, tagset=train.tagset)\n","\n","print(f\"Vocabulary size: {len(train.vocab)} words. Tag set size: {len(train.tagset)}.\")"],"metadata":{"id":"E_WvP6qQXv9R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"maHAotm7rtTg"},"source":["Now let's design the architecture of our bidirectional stacked LSTM-based POS tagger. It should consist of three layers:\n","\n","* [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) (`LSTMTagger.embed`): projecting input token IDs into its embedding space.\n","* [(Bi-)LSTM hidden state layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) (`LSTMTagger.lstm`):\n","* [Output layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (`LSTMTagger.output`): converting the hidden states to POS predictions.\n","\n","Complete the code below to define the 3 components. Use the `batch_first=True` arguement for the LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DP60pf4ByqPs"},"outputs":[],"source":["class LSTMTagger(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, tag_size, device):\n","        super(LSTMTagger, self).__init__()\n","        self.device = device\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.input_size = embedding_dim\n","\n","        ####################################\n","        #   Your code here\n","        ####################################\n","\n","        ####################################\n","\n","    def forward(self, x):\n","        # Embed the word IDs. The input to the LSTM should be:\n","        # [batch_size = 1, seq_len, input_size]\n","        x = self.embed(x).view(1, -1, self.input_size)\n","\n","        # Input all words into the LSTM and get their respective outputs\n","        # The LSTM output should be [batch_size, seq_len, hidden_size * 2]\n","        lstm_output = self.lstm(x)[0]\n","\n","        # Predict the tag for each word\n","        outputs = self.output(lstm_output.view(-1, self.hidden_size * 2))\n","        tags_probs = F.log_softmax(outputs, dim=1)\n","\n","        return tags_probs"]},{"cell_type":"markdown","metadata":{"id":"VQwufkRpr432"},"source":["Let's define the hyper-parameters initialize the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zpe1I2clIkV"},"outputs":[],"source":["num_layers = 2\n","embed_size = 128\n","intermediate_size = 256\n","vocab_size = len(train.vocab)\n","tag_size = len(train.tagset)\n","\n","num_epochs = 2\n","learning_rate = 0.002\n","device = 0 if torch.cuda.is_available() else 'cpu'\n","criterion = nn.CrossEntropyLoss()\n","\n","model = LSTMTagger(vocab_size, embed_size, intermediate_size, num_layers, tag_size, device)\n","model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"YN2ZI8wEr90x"},"source":["We can now train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLkBijlBlJTg"},"outputs":[],"source":["for epoch in range(num_epochs):\n","    losses = []\n","\n","    for inputs, targets in tqdm.notebook.tqdm(train.data):\n","        inputs = torch.from_numpy(np.array(inputs)).to(device)\n","        targets = torch.from_numpy(np.array(targets)).to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.reshape(-1))\n","        losses.append(loss.item())\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","    # Compute accuracy on the validation set\n","    preds = []\n","    targets = []\n","\n","    for inputs, curr_targets in tqdm.notebook.tqdm(dev.data):\n","      inputs = torch.from_numpy(np.array(inputs)).to(device)\n","      outputs = model(inputs)\n","      curr_preds = torch.argmax(outputs, dim=-1)\n","      preds += curr_preds.detach().cpu().numpy().tolist()\n","      targets += curr_targets\n","\n","    f1 = f1_score(targets, preds, average='macro')\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(losses):.4f}, F1 score: {f1:5.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"SXCXOQx9sFb-"},"source":["We can now test the trained model on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NGBnD9cuSxgT"},"outputs":[],"source":["preds = []\n","targets = []\n","\n","for inputs, curr_targets in tqdm.notebook.tqdm(test.data):\n","  inputs = torch.from_numpy(np.array(inputs)).to(device)\n","  outputs = model(inputs)\n","  curr_preds = torch.argmax(outputs, dim=-1)\n","  preds += curr_preds.detach().cpu().numpy().tolist()\n","  targets += curr_targets\n","\n","f1 = f1_score(targets, preds, average='macro')\n","print(f\"Test F1 score: {f1:5.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"37a9aRkBaKvq"},"source":["Finally, let's print a confusion matrix to see which tags are confused with which other tags. Look at the resulting matrix and see whether you can explain these errors."]},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix\n","\n","plt.figure(figsize=(8, 8))\n","tagset = [train.tagset.idx2word()[i] for i in range(len(train.tagset))]\n","\n","# Compute the confusion matrix\n","cm = confusion_matrix(targets, preds)\n","\n","# Create a heatmap of the confusion matrix\n","sns.heatmap(cm, annot=False, cmap='Blues', fmt='g', xticklabels=tagset, yticklabels=tagset)\n","\n","# Set the axis labels and title\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","plt.show()"],"metadata":{"id":"e0fTSi5WNUd5"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}