{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Rv18UaRljJX"
   },
   "source": [
    "## Neural and N-Gram Language Model for CPSC 503 Assignment 2\n",
    "#### The neural model notebook is modified from Yunjey Choi's Github repository - pytorch-tutorial.\n",
    "#### Familiarize yourself with pytorch, start with: https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "\n",
    "#### The N-gram model notebook is from Josh Loehr's Github repository - ngram-language-model.\n",
    "#### https://github.com/joshualoehr/ngram-language-model\n",
    "\n",
    "#### ========================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4otuXYsljJb"
   },
   "source": [
    "### Let's load a number of dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7t9hPaLcljJd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# check if GPU is available to pytorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhWxwA6JljJf"
   },
   "source": [
    "### Here are two classes needed for the data loading and formating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ljQ1bGojljJg"
   },
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    # use to generate and return the word-to-index (index-to-word) vocabulary dictionary\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def return_dict(self):\n",
    "        return self.idx2word\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    # load and prepare the corpus  the language models input format\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, n_gram=2):\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0;\n",
    "            sample_list = []\n",
    "            for line in f:\n",
    "                # add <start> tokens based on the number of n-grams.\n",
    "                words = ['<start>'] * (n_gram - 1) + line.split() + ['<end>']\n",
    "                tokens += len(words)\n",
    "                sample_list.append(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "\n",
    "        # # Read corpus and store the each line (word sequence) into its corresponding index sequence.\n",
    "        ids_list = [[0]*len(s) for s in sample_list if len(s) > n_gram] \n",
    "        with open(path, 'r') as f:\n",
    "            sample_num = 0\n",
    "            for line in f:\n",
    "                token = 0\n",
    "                words = ['<start>'] * (n_gram - 1) + line.split() + ['<end>']\n",
    "                if len(words) >= n_gram:\n",
    "                    for word in words:\n",
    "                        ids_list[sample_num][token] = self.dictionary.word2idx[word]\n",
    "                        token += 1\n",
    "                    sample_num += 1\n",
    "\n",
    "        # FOR THE NEURAL MODEL Convert the flat index sequences into the n-gram tensors which are ready for neural model.\n",
    "        for n in range(len(ids_list)):\n",
    "            flat_ids = ids_list[n]\n",
    "            ids_list[n] = torch.LongTensor([flat_ids[i:i+n_gram] for i in range(len(flat_ids)-(n_gram - 1))])\n",
    "        return ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUUKQV6iljJi"
   },
   "source": [
    "### Hyper-parameters for both Language Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BzoWts0KljJi"
   },
   "outputs": [],
   "source": [
    "# Trigram model\n",
    "n_gram = 2\n",
    "\n",
    "# m_gram is the number of preceding/conditioning words \n",
    "m_gram = n_gram - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyClyPZDljJk"
   },
   "source": [
    "### Load the \"Penn Treebank\" dataset and split it into train/dev/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wmZBKmqjljJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 31551 train, 6310 dev, 4207 test\n",
      "698238\n",
      "tensor([[ 0,  1],\n",
      "        [ 1,  2],\n",
      "        [ 2,  3],\n",
      "        [ 3,  4],\n",
      "        [ 4,  5],\n",
      "        [ 5,  6],\n",
      "        [ 6,  7],\n",
      "        [ 7,  8],\n",
      "        [ 8,  9],\n",
      "        [ 9, 10],\n",
      "        [10, 11],\n",
      "        [11, 12],\n",
      "        [12, 13],\n",
      "        [13, 14],\n",
      "        [14, 15],\n",
      "        [15, 16],\n",
      "        [16, 17],\n",
      "        [17, 18],\n",
      "        [18, 19],\n",
      "        [19, 20],\n",
      "        [20, 21],\n",
      "        [21, 22],\n",
      "        [22, 23],\n",
      "        [23, 24],\n",
      "        [24, 25]])\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus()\n",
    "#ids = corpus.get_data('data/train_mini.txt', n_gram)\n",
    "ids = corpus.get_data('data/train.txt', n_gram)\n",
    "\n",
    "\n",
    "# Use 70% for training, 15% for development, and 15% for testing \n",
    "n_train = round(len(ids) * .75)\n",
    "n_dev = round(len(ids) * .15)\n",
    "\n",
    "train_ids = ids[:n_train]\n",
    "dev_ids = ids[n_train:n_train + n_dev]\n",
    "test_ids = ids[n_train + n_dev:]\n",
    "\n",
    "print(f\"Number of sentences: {len(train_ids)} train, {len(dev_ids)} dev, {len(test_ids)} test\")\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSgcuuCAljJm"
   },
   "source": [
    "### The class of count-based language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "iuKg7_wqljJm"
   },
   "outputs": [],
   "source": [
    "class CountLM(object):\n",
    "    def __init__(self, vocab_size, x_n, x_m, laplace=1):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.laplace = laplace\n",
    "        \n",
    "        # Dictionaries for tracking the count of n-grams\n",
    "        self.n_gram_count = self.count_ngrams(x_n)\n",
    "        self.m_gram_count = self.count_ngrams(x_m)\n",
    "    \n",
    "    def count_ngrams(self, x):\n",
    "        \"\"\"\n",
    "        Populate the dictionary with the number of occurrences of each n-gram\n",
    "        \"\"\"\n",
    "        count_list = defaultdict(int)\n",
    "        for example in x:\n",
    "            for n_gram in example.tolist():\n",
    "                count_list[tuple(n_gram)] += 1 \n",
    "        return count_list\n",
    "    \n",
    "    def compute_mle(self, n_gram):\n",
    "        \"\"\"\n",
    "        Compute the MLE of P(w_n|w_{n−1}, ...) with add-one Laplacian smoothing\n",
    "        \n",
    "        Please see chapter 3.5.1 of J&M 3rd Ed. for more information\n",
    "        \"\"\"\n",
    "        n_count = self.n_gram_count[n_gram]\n",
    "        m_gram = n_gram[:-1]\n",
    "        m_count = self.m_gram_count[m_gram]\n",
    "        prob = (n_count + self.laplace) / (m_count + self.laplace * self.vocab_size) \n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lixRBxehljJy"
   },
   "source": [
    "### Train the n-gram model based on MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YFIiHH-ljJy",
    "outputId": "090cec1f-de89-4361-912a-9fdfc00a9b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average training perplexity for count-based LM: 872.2299472899585\n"
     ]
    }
   ],
   "source": [
    "#m_gram_train_ids = corpus.get_data('data/train_mini.txt', m_gram)[:-200]\n",
    "m_gram_train_ids = corpus.get_data('data/train.txt', m_gram)[:n_train]\n",
    "n_gram_train_ids = train_ids\n",
    "\n",
    "# Populate the dictionaries with counts from training corpus\n",
    "count_model = CountLM(vocab_size, n_gram_train_ids, m_gram_train_ids)\n",
    "\n",
    "# Compute average perplexity on training set\n",
    "train_ppl = 0\n",
    "for i in range(0, len(train_ids)):\n",
    "\n",
    "    probabilities = list(map(lambda x: count_model.compute_mle(tuple(x)), train_ids[i].tolist()))\n",
    "    perplexity = np.exp(sum(-np.log(probabilities)) / len(train_ids[i]))\n",
    "    train_ppl += perplexity\n",
    "    \n",
    "print('The average training perplexity for count-based LM: '+str(train_ppl/len(train_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qSmtg864ljJ0",
    "outputId": "894898a7-d2fa-4c40-8fb4-3dcc785dfafd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average testing perplexity for count-based LM: 1184.6602509874976\n"
     ]
    }
   ],
   "source": [
    "# Compute average perplexity on testing set\n",
    "test_ppl = 0\n",
    "for i in range(0, len(test_ids)):\n",
    "\n",
    "    probabilities = list(map(lambda x: count_model.compute_mle(tuple(x)), test_ids[i].tolist()))\n",
    "    perplexity = np.exp(sum(-np.log(probabilities)) / len(test_ids[i]))\n",
    "    print('Perplexity for test sample '+str(i)+' :', perplexity)\n",
    "    test_ppl += perplexity\n",
    "    \n",
    "print('The average testing perplexity for count-based LM: '+str(test_ppl/len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters for the neural language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR THE NEURAL MODEL\n",
    "embed_size = 128\n",
    "intermediate_size = 1024\n",
    "num_epochs = 2\n",
    "learning_rate = 2e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cl7DhAIljJ0"
   },
   "source": [
    "### The class of neural language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JZi51vWzljJ1"
   },
   "outputs": [],
   "source": [
    "class NeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, intermediate_size, m_gram):\n",
    "        super(NeuralLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.intermediate = nn.Linear(m_gram * embed_size, intermediate_size)\n",
    "        self.final = nn.Linear(intermediate_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # Embed word id(s) to vectors\n",
    "        conc_emb = x.view(x.size(0), x.size(1)*x.size(2))\n",
    "        intermediate_output = self.intermediate(conc_emb) # one layer of MLP\n",
    "        intermediate_output = F.relu(intermediate_output) # ReLU non-linear function\n",
    "        final_out = self.final(intermediate_output) # Map to the vocabulary size output\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gK_WyjiOljJ1"
   },
   "outputs": [],
   "source": [
    "neural_model = NeuralLM(vocab_size, embed_size, intermediate_size, m_gram).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(neural_model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFfNEeZrljJ1"
   },
   "source": [
    "### Train the neural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jb-yeBUKljJ2",
    "outputId": "aae5583e-5776-4c4f-a712-646ac39c840c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10910/10910 [00:25<00:00, 423.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Training Loss: 2.1705, Dev Perplexity: 539.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10910/10910 [00:25<00:00, 422.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Training Loss: 2.0152, Dev Perplexity: 472.67\n"
     ]
    }
   ],
   "source": [
    "# Reduce batch size if you are running out of memory\n",
    "batch_size = 64\n",
    "training_data = torch.cat(train_ids, dim=0)\n",
    "neural_model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i in tqdm(range(0, len(training_data), batch_size)):\n",
    "        batch = training_data[i:i + batch_size]\n",
    "        inputs = batch[:, 0:n_gram-1].to(device)\n",
    "        targets = batch[:, n_gram-1:].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = neural_model(inputs)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        total_loss += loss.item();\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    # Calculate the performance (perplexity) of the current trained model on dev set.\n",
    "    total_ppl = 0\n",
    "    for i in range(0, len(dev_ids)):\n",
    "        dev_inputs = dev_ids[i][:, 0:n_gram-1].to(device)\n",
    "        dev_targets = dev_ids[i][:, n_gram-1:].to(device)\n",
    "        dev_outputs = neural_model(dev_inputs)\n",
    "        ce = criterion(dev_outputs, dev_targets.reshape(-1))\n",
    "        total_ppl += np.exp(ce.item());\n",
    "    \n",
    "    print ('Epoch [{}/{}], Training Loss: {:.4f}, Dev Perplexity: {:5.2f}'\n",
    "        .format(epoch + 1, num_epochs, total_loss/len(train_ids), total_ppl/len(dev_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCbcvvDwljJ2"
   },
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "WEU68Mc0ljJ3",
    "outputId": "e7200000-2789-4468-929d-d82081f840d7"
   },
   "outputs": [],
   "source": [
    "torch.save(neural_model, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRvZAdgSljJ3"
   },
   "source": [
    "### Neural Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKu9d0lLljJ3",
    "outputId": "ea522406-6c9f-4316-b696-28eb574b1385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average testing perplexity for neural LM: 449.3337552729946\n"
     ]
    }
   ],
   "source": [
    "neural_model = torch.load('model.ckpt')\n",
    "neural_model.eval()\n",
    "test_ppl = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_ids)):\n",
    "        inputs = test_ids[i][:, 0:n_gram-1].to(device)\n",
    "        gold = test_ids[i][:, n_gram-1:].to(device)\n",
    "        output = neural_model(inputs)\n",
    "        cross_entropy = criterion(output, gold.reshape(-1)).item()\n",
    "        print('Perplexity for test sample '+str(i)+' :', np.exp(cross_entropy))\n",
    "        test_ppl += np.exp(cross_entropy)\n",
    "    print('The average testing perplexity for neural LM: '+str(test_ppl/len(test_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYakMBnpljJ4"
   },
   "source": [
    "### Text generation with n-gram language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from count-based language model:\n",
      "we keep up\n",
      "a life including the pentagon in the buy-back is n't be an early termination by the indictment against index tumble quickly tumbled N billion\n",
      "they are less <unk> on it 's one of luzon\n",
      "\n",
      "Generating from neural language model:\n",
      "we to performed idea anything price you <unk> a business publicly was in east tailspin gain to move at a\n",
      "a rate\n",
      "they can be able to N southeast misleading bills including to N N N N tons machinists much return to\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "start_tokens = ['we', 'a', 'they']\n",
    "\n",
    "\n",
    "print('Generating from count-based language model:')\n",
    "for token in start_tokens:\n",
    "    \n",
    "    sentence = [token]\n",
    "    token_id = corpus.dictionary.word2idx[token]\n",
    "    \n",
    "    context = tuple([corpus.dictionary.word2idx['<start>'] for _ in range(m_gram - 1)] + [token_id])\n",
    "    while sentence[-1] != '<end>':\n",
    "        filtered_ngrams = [(n_gram_ids[-1], count) for n_gram_ids, count in count_model.n_gram_count.items() if n_gram_ids[:-1] == context]\n",
    "        candidates = [x[0] for x in filtered_ngrams]\n",
    "        total_count = sum([x[1] for x in filtered_ngrams])\n",
    "        probs = [x[1] / total_count for x in filtered_ngrams]\n",
    "    \n",
    "        sampled_id = np.random.choice(candidates, 1, p=probs)[0]\n",
    "        sampled_token = corpus.dictionary.idx2word[sampled_id]\n",
    "        sentence.append(sampled_token)\n",
    "        \n",
    "        context = context[1:] + (sampled_id,)\n",
    "    print(' '.join(sentence[:-1]))\n",
    "\n",
    "    \n",
    "neural_model.eval()\n",
    "print('\\nGenerating from neural language model:')\n",
    "for token in start_tokens:\n",
    "    \n",
    "    sentence = [token]\n",
    "    token_id = corpus.dictionary.word2idx[token]\n",
    "    \n",
    "    context = torch.LongTensor([[corpus.dictionary.word2idx['<start>'] for _ in range(m_gram - 1)] + [token_id]]).to(device)\n",
    "    \n",
    "    while sentence[-1] != '<end>':\n",
    "        output = neural_model(context)\n",
    "        softmax_output = F.softmax(output, dim=-1)\n",
    "        prob = softmax_output.squeeze(0).tolist()\n",
    "        prob = list(map(lambda x: x / sum(prob), prob))\n",
    "        sampled_id = np.random.choice(output.shape[1], 1, p=prob)[0]\n",
    "        sampled_token = corpus.dictionary.idx2word[sampled_id]\n",
    "        sentence.append(sampled_token)\n",
    "        context = torch.cat([context[:, 1:], torch.LongTensor([[sampled_id]]).to(device)], dim=1)\n",
    "    print(' '.join(sentence[:-1]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Language_model_with_ngram (Raymond+ Giuseppe).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
